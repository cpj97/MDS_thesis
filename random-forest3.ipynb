{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roandom Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:11.635852Z",
     "iopub.status.busy": "2025-03-12T09:07:11.635460Z",
     "iopub.status.idle": "2025-03-12T09:07:19.489740Z",
     "shell.execute_reply": "2025-03-12T09:07:19.488687Z",
     "shell.execute_reply.started": "2025-03-12T09:07:11.635823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install panelsplit\n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:19.491696Z",
     "iopub.status.busy": "2025-03-12T09:07:19.491359Z",
     "iopub.status.idle": "2025-03-12T09:07:19.497614Z",
     "shell.execute_reply": "2025-03-12T09:07:19.496644Z",
     "shell.execute_reply.started": "2025-03-12T09:07:19.491669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import random\n",
    "\n",
    "from panelsplit.cross_validation import PanelSplit\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import openpyxl\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit  # Alternative to PanelSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check working directory\n",
    "#print(os.getcwd()) \n",
    "\n",
    "### Define file and path\n",
    "file_path = r\"c:\\Users\\mmier\\OneDrive - Hertie School\\3. Estudio\\2025 MDS\\2025-1 MDS Thesis\\MDS_thesis\\Data\\OSC\" #Use a raw string (r\"\") when defining paths\n",
    "file = \"Datos-ICM-2023.xlsx\"\n",
    "full_path = os.path.join(file_path, file)\n",
    "\n",
    "### List files in directory\n",
    "#print(os.listdir(file_path))\n",
    "\n",
    "### Load excel file\n",
    "df = pd.read_excel(full_path, engine=\"openpyxl\")\n",
    "\n",
    "# Display the first few rows\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. Correct df format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename columns names with row 4\n",
    "df.columns = df.iloc[3]\n",
    "\n",
    "### Delete first (index 0) and third (index 2) row\n",
    "df = df.drop([0, 1, 2, 3], axis=0)\n",
    "\n",
    "### Reset index \n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Define train, test and evaluation set\n",
    "\n",
    "Evaluation set: 2019 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Separate evaluation set \n",
    "final_df = df[df[\"AÃ‘O\"] < 2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define y and X1\n",
    "\n",
    "y: deforestation\n",
    "\n",
    "M-03-25\tHectÃ¡reas de bosque deforestadas\n",
    "\n",
    "X1: general variables\n",
    "\n",
    "AglomeraciÃ³n\n",
    "SC FunciÃ³n ciudades\n",
    "Departamento\n",
    "Municipio\n",
    "Divipola\n",
    "AÃ‘O\n",
    "ICM-00-0\tÃndice de Ciudades Modernas\n",
    "PCC-00-0\tÃndice de Productividad, Competitividad y Complementariedad EconÃ³mica\n",
    "GPI-00-0\tÃndice de Gobernanza, ParticipaciÃ³n e Instituciones\n",
    "EIS-00-0\tÃndice de Equidad e inclusiÃ³n social\n",
    "CTI-00-0\tÃndice de Ciencia, TecnologÃ­a e InnovaciÃ³n\n",
    "SEG-00-0\tÃndice de Seguridad\n",
    "SOS-00-0\tÃndice de Sostenibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define y\n",
    "y = final_df[\"M-03-25\"]\n",
    "\n",
    "### Define X1: general variables \n",
    "X1 = final_df[[\"AglomeraciÃ³n\", \"SC FunciÃ³n ciudades\", \"Departamento\", \"Municipio\", \"Divipola\", \"AÃ‘O\",\"ICM-00-0\", \"PCC-00-0\", \"GPI-00-0\", \"EIS-00-0\", \"CTI-00-0\", \"SEG-00-0\", \"SOS-00-0\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Remove NAS and order by \"Municipio\" and \"AÃ‘O\"\n",
    "\n",
    "Prevents future values from being included in training accidentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:23.925137Z",
     "iopub.status.busy": "2025-03-12T09:07:23.924870Z",
     "iopub.status.idle": "2025-03-12T09:07:24.253388Z",
     "shell.execute_reply": "2025-03-12T09:07:24.252429Z",
     "shell.execute_reply.started": "2025-03-12T09:07:23.925116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Check for missing values\n",
    "#print(f\"Missing values in X1:\\n{X1.isnull().sum().sum()}\")\n",
    "#print(f\"Missing values in y:\\n{y.isnull().sum().sum()}\")\n",
    "\n",
    "### Remove NAS\n",
    "df_combined = pd.concat([X1, y], axis=1)  # Combine into one DataFrame\n",
    "df_combined.dropna(inplace=True)  # Remove rows with any NaN values\n",
    "df_combined = df_combined.reset_index(drop=True) # Reset index \n",
    "\n",
    "### Organize by municipality and year for time series split \n",
    "df_combined = df_combined.sort_values(by=[\"Municipio\", \"AÃ‘O\"]).reset_index(drop=True)\n",
    "\n",
    "### Separate again\n",
    "X1 = df_combined.iloc[:, :-1]  # All columns except the last (features)\n",
    "y = df_combined.iloc[:, -1]    # The last column (target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Encode for  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n",
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n",
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n",
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n",
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "for col in [\"AglomeraciÃ³n\", \"SC FunciÃ³n ciudades\", \"Departamento\", \"Municipio\", \"Divipola\"]:\n",
    "    le = LabelEncoder()\n",
    "    X1[col] = le.fit_transform(X1[col])\n",
    "    \n",
    "print(y.dtype)\n",
    "print(X1[\"AglomeraciÃ³n\"].dtype)\n",
    "print(X1[\"SC FunciÃ³n ciudades\"].dtype)\n",
    "print(X1[\"Departamento\"].dtype)\n",
    "print(X1[\"Municipio\"].dtype)\n",
    "print(X1[\"Divipola\"].dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Create lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, test_idx \u001b[38;5;129;01min\u001b[39;00m tscv.split(X1):\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m## Split dataset into train & test per fold\u001b[39;00m\n\u001b[32m     26\u001b[39m     X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     y_train, y_test = \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m[train_idx], y.iloc[test_idx]\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m## Train and evaluate for each combination of hyperparameters\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m n_estimators \u001b[38;5;129;01min\u001b[39;00m n_estimators_values:\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "seed_value = 17\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "### Define Time-Series Cross-Validation (5 splits)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "### Hyperparameters for Random Forest\n",
    "n_estimators_values = [50, 100, 200, 300]  # Number of trees to test\n",
    "max_depth_values = [5, 10, 15, 20, 30, None]  # Depth of trees\n",
    "\n",
    "### Function to compute Adjusted RÂ²\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "### Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "\n",
    "\n",
    "### Perform Time-Series Cross-Validation (iterate over each split)\n",
    "for train_idx, test_idx in tscv.split(X1):\n",
    "\n",
    "    ## Split dataset into train & test per fold\n",
    "    X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    ## Train and evaluate for each combination of hyperparameters\n",
    "    for n_estimators in n_estimators_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            \n",
    "            # Define the Random Forest model\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=seed_value,\n",
    "                n_jobs=-1  # Use all available processors\n",
    "            )\n",
    "            # Train the model\n",
    "            model.fit(X1_train, y_train.ravel())\n",
    "\n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X1_train)\n",
    "            y_test_pred = model.predict(X1_test)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            mse = mean_squared_error(y_test, y_test_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "            # Compute Adjusted RÂ²\n",
    "            n_train, k = X1_train.shape\n",
    "            n_test = X1_test.shape[0]\n",
    "            adj_r2_train = adjusted_r2(r2_train, n_train, k)\n",
    "            adj_r2_test = adjusted_r2(r2_test, n_test, k)\n",
    "\n",
    "            # Store results for this combination\n",
    "            results[(n_estimators, max_depth)] = {\n",
    "                \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2_test\": r2_test, \"R2_train\": r2_train,\n",
    "                \"Adj_R2_test\": adj_r2_test, \"Adj_R2_train\": adj_r2_train\n",
    "            }\n",
    "            \n",
    "### Find the best hyperparameter combination (minimize MSE, maximize RÂ²)\n",
    "best_params = min(results, key=lambda x: (results[x][\"MSE\"], -results[x][\"R2_test\"]))\n",
    "best_metrics = results[best_params]\n",
    "\n",
    "### Print optimal hyperparameters and their performance\n",
    "print(f\"ðŸŒ² Optimal Random Forest Parameters: n_estimators={best_params[0]}, max_depth={best_params[1]}\")\n",
    "print(f\"ðŸ“Š Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"ðŸ“Š Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"ðŸ“Š Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"ðŸ“Š RÂ² (Train): {best_metrics['R2_train']:.4f}, Adjusted RÂ² (Train): {best_metrics['Adj_R2_train']:.4f}\")\n",
    "print(f\"ðŸ“Š RÂ² (Test): {best_metrics['R2_test']:.4f}, Adjusted RÂ² (Test): {best_metrics['Adj_R2_test']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1686, Test size: 1681\n",
      "Train size: 3367, Test size: 1681\n",
      "Train size: 5048, Test size: 1681\n",
      "Train size: 6729, Test size: 1681\n",
      "Train size: 8410, Test size: 1681\n",
      "ðŸŒ² Optimal Random Forest Parameters: n_estimators=50, max_depth=5\n",
      "ðŸ“Š Best MSE: 891004.4595\n",
      "ðŸ“Š Best RMSE: 908.7719\n",
      "ðŸ“Š Best MAE: 191.9636\n",
      "ðŸ“Š RÂ² (Train): 0.4441, Adjusted RÂ² (Train): 0.4432\n",
      "ðŸ“Š RÂ² (Test): -0.0143, Adjusted RÂ² (Test): -0.0186\n"
     ]
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "seed_value = 17\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "### Define Time-Series Cross-Validation (5 splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5, max_train_size=None)\n",
    "\n",
    "### Hyperparameters for Random Forest\n",
    "n_estimators_values = [50, 100, 200, 300]  # Number of trees to test\n",
    "max_depth_values = [5, 10, 20, None]  # Depth of trees\n",
    "\n",
    "\n",
    "### Function to compute Adjusted RÂ²\n",
    "def adjusted_r2(r2, n, k):\n",
    "    if n <= k + 1:\n",
    "        return np.nan  # Avoid division by zero\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "### Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "### Perform Time-Series Cross-Validation\n",
    "for train_idx, test_idx in tscv.split(X1):\n",
    "    print(f\"Train size: {len(train_idx)}, Test size: {len(test_idx)}\")  # Debug check\n",
    "\n",
    "    ## Split dataset into train & test per fold (using `.loc[]` for safe indexing)\n",
    "    X1_train, X1_test = X1.loc[X1.index[train_idx]], X1.loc[X1.index[test_idx]]\n",
    "    y_train, y_test = y.loc[y.index[train_idx]], y.loc[y.index[test_idx]]\n",
    "\n",
    "    ## Train and evaluate for each combination of hyperparameters\n",
    "    for n_estimators in n_estimators_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            \n",
    "            # Define the Random Forest model\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=seed_value,\n",
    "                n_jobs=-1  # Use all available processors\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X1_train, y_train)\n",
    "\n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X1_train)\n",
    "            y_test_pred = model.predict(X1_test)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            mse = mean_squared_error(y_test, y_test_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "            # Compute Adjusted RÂ²\n",
    "            n_train, k = X1_train.shape\n",
    "            n_test = X1_test.shape[0]\n",
    "            adj_r2_train = adjusted_r2(r2_train, n_train, k)\n",
    "            adj_r2_test = adjusted_r2(r2_test, n_test, k)\n",
    "\n",
    "            # Store results for this combination (append results for all folds)\n",
    "            if (n_estimators, max_depth) not in results:\n",
    "                results[(n_estimators, max_depth)] = []\n",
    "\n",
    "            results[(n_estimators, max_depth)].append({\n",
    "                \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2_test\": r2_test, \"R2_train\": r2_train,\n",
    "                \"Adj_R2_test\": adj_r2_test, \"Adj_R2_train\": adj_r2_train\n",
    "            })\n",
    "\n",
    "### Compute Average Performance Across Folds\n",
    "averaged_results = {\n",
    "    params: {metric: np.mean([fold[metric] for fold in folds])\n",
    "             for metric in folds[0].keys()}\n",
    "    for params, folds in results.items()\n",
    "}\n",
    "\n",
    "### Find the best hyperparameter combination (minimize MSE, maximize RÂ²)\n",
    "best_params = min(averaged_results, key=lambda x: (averaged_results[x][\"MSE\"], -averaged_results[x][\"R2_test\"]))\n",
    "best_metrics = averaged_results[best_params]\n",
    "\n",
    "### Print optimal hyperparameters and their performance\n",
    "print(f\"ðŸŒ² Optimal Random Forest Parameters: n_estimators={best_params[0]}, max_depth={best_params[1]}\")\n",
    "print(f\"ðŸ“Š Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"ðŸ“Š Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"ðŸ“Š Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"ðŸ“Š RÂ² (Train): {best_metrics['R2_train']:.4f}, Adjusted RÂ² (Train): {best_metrics['Adj_R2_train']:.4f}\")\n",
    "print(f\"ðŸ“Š RÂ² (Test): {best_metrics['R2_test']:.4f}, Adjusted RÂ² (Test): {best_metrics['Adj_R2_test']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Implement GroupKFold for Panel Data\n",
    "\n",
    "We need to group by \"Municipio\" when performing cross-validation.\n",
    "\n",
    "- Ensures that all data from one municipality is only in train or test (not both).\n",
    "- Respects time structure by sorting by \"AÃ‘O\" first.\n",
    "- Prevents municipalities from leaking into both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8075, Test size: 2016\n",
      "Train size: 8075, Test size: 2016\n",
      "Train size: 8069, Test size: 2022\n",
      "Train size: 8076, Test size: 2015\n",
      "Train size: 8069, Test size: 2022\n"
     ]
    }
   ],
   "source": [
    "# Ensure y is a Pandas Series (not a NumPy array)\n",
    "if isinstance(y, np.ndarray):  \n",
    "    y = pd.Series(y, index=X1.index)  # Restore indexing\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Define the number of splits\n",
    "n_splits = 5  \n",
    "tscv = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "# Extract groups (Municipio column as identifier)\n",
    "groups = X1[\"Municipio\"].values  \n",
    "\n",
    "for train_idx, test_idx in tscv.split(X1, y, groups=groups):\n",
    "    X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    print(f\"Train size: {len(train_idx)}, Test size: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Train a Quick Random Forest to Identify Important Features\n",
    "\n",
    "Before running the full model, we train a baseline Random Forest to rank feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature  Importance\n",
      "0  ICM-00-0    0.395368\n",
      "6  SOS-00-0    0.166915\n",
      "1  PCC-00-0    0.109436\n",
      "2  GPI-00-0    0.103235\n",
      "3  EIS-00-0    0.090039\n",
      "5  SEG-00-0    0.069330\n",
      "4  CTI-00-0    0.065677\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Define a baseline Random Forest model\n",
    "rf_baseline = RandomForestRegressor(\n",
    "    n_estimators=50,  # Small number of trees for quick evaluation\n",
    "    max_depth=10,     # Limit tree depth to avoid overfitting\n",
    "    random_state=17,\n",
    "    n_jobs=-1         # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Fit the model on the full dataset (ignoring \"AÃ‘O\" and \"Municipio\" for now)\n",
    "X1_selected = X1.drop(columns=[\"AglomeraciÃ³n\", \"SC FunciÃ³n ciudades\", \"Departamento\", \"Municipio\", \"Divipola\", \"AÃ‘O\"])  # Remove non-predictive ID variables\n",
    "rf_baseline.fit(X1_selected, y)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X1_selected.columns,\n",
    "    \"Importance\": rf_baseline.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display the top 30 features\n",
    "print(feature_importance_df.head(30))\n",
    "#import ace_tools as tools\n",
    "#tools.display_dataframe_to_user(name=\"Top Feature Importance\", dataframe=feature_importance_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39536799, 0.10943638, 0.10323458, 0.09003945, 0.06567688,\n",
       "       0.06932975, 0.16691497])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the top 30 most important features\n",
    "#top_features = feature_importance_df[\"Feature\"].head(30).tolist()\n",
    "\n",
    "# Update X1 to only include the top features\n",
    "#X1 = X1[top_features]\n",
    "\n",
    "#print(f\"âœ… Selected top {len(top_features)} features for the final model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Train the Final Random Forest Model\n",
    "\n",
    "Train Random Forest with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ðŸŒ² Best Random Forest Parameters: n_estimators=300, max_depth=None\n",
      "ðŸ“Š Best MSE: 1285427.8743\n",
      "ðŸ“Š Best RMSE: 1133.7549\n",
      "ðŸ“Š Best MAE: 877.6061\n",
      "ðŸ“Š RÂ² (Train): 0.9542\n",
      "ðŸ“Š RÂ² (Test): 0.6917\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for tuning\n",
    "n_estimators_values = [50, 100, 200, 300]  # Number of trees\n",
    "max_depth_values = [5, 10, 20, None]       # Depth of trees\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Perform Time-Series Cross-Validation\n",
    "from sklearn.model_selection import GroupKFold\n",
    "tscv = GroupKFold(n_splits=5)\n",
    "groups = X1.index  # Using index since Municipio was dropped\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X1, y, groups=groups):\n",
    "    X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train and evaluate for each combination of hyperparameters\n",
    "    for n_estimators in n_estimators_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            \n",
    "            # Define the Random Forest model\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=17,\n",
    "                n_jobs=-1  # Use all available processors\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X1_train, y_train)\n",
    "\n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X1_train)\n",
    "            y_test_pred = model.predict(X1_test)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "            mse = mean_squared_error(y_test, y_test_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "            # Store results\n",
    "            if (n_estimators, max_depth) not in results:\n",
    "                results[(n_estimators, max_depth)] = []\n",
    "\n",
    "            results[(n_estimators, max_depth)].append({\n",
    "                \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2_test\": r2_test, \"R2_train\": r2_train\n",
    "            })\n",
    "\n",
    "### Compute Average Performance Across Folds\n",
    "averaged_results = {\n",
    "    params: {metric: np.mean([fold[metric] for fold in folds])\n",
    "             for metric in folds[0].keys()}\n",
    "    for params, folds in results.items()\n",
    "}\n",
    "\n",
    "### Find the best hyperparameter combination\n",
    "best_params = min(averaged_results, key=lambda x: (averaged_results[x][\"MSE\"], -averaged_results[x][\"R2_test\"]))\n",
    "best_metrics = averaged_results[best_params]\n",
    "\n",
    "### Print optimal hyperparameters and performance\n",
    "print(f\"ðŸŒ² Best Random Forest Parameters: n_estimators={best_params[0]}, max_depth={best_params[1]}\")\n",
    "print(f\"ðŸ“Š Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"ðŸ“Š Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"ðŸ“Š Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"ðŸ“Š RÂ² (Train): {best_metrics['R2_train']:.4f}\")\n",
    "print(f\"ðŸ“Š RÂ² (Test): {best_metrics['R2_test']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6837566,
     "sourceId": 10985952,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "MDS_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
