{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roandom Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:11.635852Z",
     "iopub.status.busy": "2025-03-12T09:07:11.635460Z",
     "iopub.status.idle": "2025-03-12T09:07:19.489740Z",
     "shell.execute_reply": "2025-03-12T09:07:19.488687Z",
     "shell.execute_reply.started": "2025-03-12T09:07:11.635823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install panelsplit\n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:19.491696Z",
     "iopub.status.busy": "2025-03-12T09:07:19.491359Z",
     "iopub.status.idle": "2025-03-12T09:07:19.497614Z",
     "shell.execute_reply": "2025-03-12T09:07:19.496644Z",
     "shell.execute_reply.started": "2025-03-12T09:07:19.491669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import random\n",
    "\n",
    "from panelsplit.cross_validation import PanelSplit\n",
    "\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import openpyxl\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit  # Alternative to PanelSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check working directory\n",
    "#print(os.getcwd()) \n",
    "\n",
    "### Define file and path\n",
    "file_path = r\"c:\\Users\\mmier\\OneDrive - Hertie School\\3. Estudio\\2025 MDS\\2025-1 MDS Thesis\\MDS_thesis\\Data\\OSC\" #Use a raw string (r\"\") when defining paths\n",
    "file = \"Datos-ICM-2023.xlsx\"\n",
    "full_path = os.path.join(file_path, file)\n",
    "\n",
    "### List files in directory\n",
    "#print(os.listdir(file_path))\n",
    "\n",
    "### Load excel file\n",
    "df = pd.read_excel(full_path, engine=\"openpyxl\")\n",
    "\n",
    "# Display the first few rows\n",
    "#print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. Correct df format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Rename columns names with row 4\n",
    "df.columns = df.iloc[3]\n",
    "\n",
    "### Delete first (index 0) and third (index 2) row\n",
    "df = df.drop([0, 1, 2, 3], axis=0)\n",
    "\n",
    "### Reset index \n",
    "df = df.reset_index(drop=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Define train, test and evaluation set\n",
    "\n",
    "Evaluation set: 2019 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Separate evaluation set \n",
    "final_df = df[df[\"AÑO\"] < 2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define y and X1\n",
    "\n",
    "y: deforestation\n",
    "\n",
    "M-03-25\tHectáreas de bosque deforestadas\n",
    "\n",
    "X1: general variables\n",
    "\n",
    "Aglomeración\n",
    "SC Función ciudades\n",
    "Departamento\n",
    "Municipio\n",
    "Divipola\n",
    "AÑO\n",
    "ICM-00-0\tÍndice de Ciudades Modernas\n",
    "PCC-00-0\tÍndice de Productividad, Competitividad y Complementariedad Económica\n",
    "GPI-00-0\tÍndice de Gobernanza, Participación e Instituciones\n",
    "EIS-00-0\tÍndice de Equidad e inclusión social\n",
    "CTI-00-0\tÍndice de Ciencia, Tecnología e Innovación\n",
    "SEG-00-0\tÍndice de Seguridad\n",
    "SOS-00-0\tÍndice de Sostenibilidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Define y\n",
    "y = final_df[\"M-03-25\"]\n",
    "\n",
    "### Define X1: general variables \n",
    "X1 = final_df[[\"Aglomeración\", \"SC Función ciudades\", \"Departamento\", \"Municipio\", \"Divipola\", \"AÑO\",\"ICM-00-0\", \"PCC-00-0\", \"GPI-00-0\", \"EIS-00-0\", \"CTI-00-0\", \"SEG-00-0\", \"SOS-00-0\"]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3. Remove NAS and order by \"Municipio\" and \"AÑO\"\n",
    "\n",
    "Prevents future values from being included in training accidentally."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:23.925137Z",
     "iopub.status.busy": "2025-03-12T09:07:23.924870Z",
     "iopub.status.idle": "2025-03-12T09:07:24.253388Z",
     "shell.execute_reply": "2025-03-12T09:07:24.252429Z",
     "shell.execute_reply.started": "2025-03-12T09:07:23.925116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "### Check for missing values\n",
    "#print(f\"Missing values in X1:\\n{X1.isnull().sum().sum()}\")\n",
    "#print(f\"Missing values in y:\\n{y.isnull().sum().sum()}\")\n",
    "\n",
    "### Remove NAS\n",
    "df_combined = pd.concat([X1, y], axis=1)  # Combine into one DataFrame\n",
    "df_combined.dropna(inplace=True)  # Remove rows with any NaN values\n",
    "df_combined = df_combined.reset_index(drop=True) # Reset index \n",
    "\n",
    "### Organize by municipality and year for time series split \n",
    "df_combined = df_combined.sort_values(by=[\"Municipio\", \"AÑO\"]).reset_index(drop=True)\n",
    "\n",
    "### Separate again\n",
    "X1 = df_combined.iloc[:, :-1]  # All columns except the last (features)\n",
    "y = df_combined.iloc[:, -1]    # The last column (target variable)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Encode for  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n",
      "int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n",
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n",
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n",
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n",
      "C:\\Users\\mmier\\AppData\\Local\\Temp\\ipykernel_21044\\3457745230.py:7: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  X1[col] = le.fit_transform(X1[col])\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import LabelEncoder\n",
    "le = LabelEncoder()\n",
    "y = le.fit_transform(y)\n",
    "\n",
    "for col in [\"Aglomeración\", \"SC Función ciudades\", \"Departamento\", \"Municipio\", \"Divipola\"]:\n",
    "    le = LabelEncoder()\n",
    "    X1[col] = le.fit_transform(X1[col])\n",
    "    \n",
    "print(y.dtype)\n",
    "print(X1[\"Aglomeración\"].dtype)\n",
    "print(X1[\"SC Función ciudades\"].dtype)\n",
    "print(X1[\"Departamento\"].dtype)\n",
    "print(X1[\"Municipio\"].dtype)\n",
    "print(X1[\"Divipola\"].dtype)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4. Create lags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Random Forest Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute 'iloc'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[24]\u001b[39m\u001b[32m, line 27\u001b[39m\n\u001b[32m     23\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m train_idx, test_idx \u001b[38;5;129;01min\u001b[39;00m tscv.split(X1):\n\u001b[32m     24\u001b[39m \n\u001b[32m     25\u001b[39m     \u001b[38;5;66;03m## Split dataset into train & test per fold\u001b[39;00m\n\u001b[32m     26\u001b[39m     X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m     y_train, y_test = \u001b[43my\u001b[49m\u001b[43m.\u001b[49m\u001b[43miloc\u001b[49m[train_idx], y.iloc[test_idx]\n\u001b[32m     29\u001b[39m     \u001b[38;5;66;03m## Train and evaluate for each combination of hyperparameters\u001b[39;00m\n\u001b[32m     30\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m n_estimators \u001b[38;5;129;01min\u001b[39;00m n_estimators_values:\n",
      "\u001b[31mAttributeError\u001b[39m: 'numpy.ndarray' object has no attribute 'iloc'"
     ]
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "seed_value = 17\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "### Define Time-Series Cross-Validation (5 splits)\n",
    "\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "### Hyperparameters for Random Forest\n",
    "n_estimators_values = [50, 100, 200, 300]  # Number of trees to test\n",
    "max_depth_values = [5, 10, 15, 20, 30, None]  # Depth of trees\n",
    "\n",
    "### Function to compute Adjusted R²\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "### Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "\n",
    "\n",
    "### Perform Time-Series Cross-Validation (iterate over each split)\n",
    "for train_idx, test_idx in tscv.split(X1):\n",
    "\n",
    "    ## Split dataset into train & test per fold\n",
    "    X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    ## Train and evaluate for each combination of hyperparameters\n",
    "    for n_estimators in n_estimators_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            \n",
    "            # Define the Random Forest model\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=seed_value,\n",
    "                n_jobs=-1  # Use all available processors\n",
    "            )\n",
    "            # Train the model\n",
    "            model.fit(X1_train, y_train.ravel())\n",
    "\n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X1_train)\n",
    "            y_test_pred = model.predict(X1_test)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            mse = mean_squared_error(y_test, y_test_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "            # Compute Adjusted R²\n",
    "            n_train, k = X1_train.shape\n",
    "            n_test = X1_test.shape[0]\n",
    "            adj_r2_train = adjusted_r2(r2_train, n_train, k)\n",
    "            adj_r2_test = adjusted_r2(r2_test, n_test, k)\n",
    "\n",
    "            # Store results for this combination\n",
    "            results[(n_estimators, max_depth)] = {\n",
    "                \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2_test\": r2_test, \"R2_train\": r2_train,\n",
    "                \"Adj_R2_test\": adj_r2_test, \"Adj_R2_train\": adj_r2_train\n",
    "            }\n",
    "            \n",
    "### Find the best hyperparameter combination (minimize MSE, maximize R²)\n",
    "best_params = min(results, key=lambda x: (results[x][\"MSE\"], -results[x][\"R2_test\"]))\n",
    "best_metrics = results[best_params]\n",
    "\n",
    "### Print optimal hyperparameters and their performance\n",
    "print(f\"🌲 Optimal Random Forest Parameters: n_estimators={best_params[0]}, max_depth={best_params[1]}\")\n",
    "print(f\"📊 Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"📊 Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"📊 Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"📊 R² (Train): {best_metrics['R2_train']:.4f}, Adjusted R² (Train): {best_metrics['Adj_R2_train']:.4f}\")\n",
    "print(f\"📊 R² (Test): {best_metrics['R2_test']:.4f}, Adjusted R² (Test): {best_metrics['Adj_R2_test']:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Correction suggested"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 1686, Test size: 1681\n",
      "Train size: 3367, Test size: 1681\n",
      "Train size: 5048, Test size: 1681\n",
      "Train size: 6729, Test size: 1681\n",
      "Train size: 8410, Test size: 1681\n",
      "🌲 Optimal Random Forest Parameters: n_estimators=50, max_depth=5\n",
      "📊 Best MSE: 891004.4595\n",
      "📊 Best RMSE: 908.7719\n",
      "📊 Best MAE: 191.9636\n",
      "📊 R² (Train): 0.4441, Adjusted R² (Train): 0.4432\n",
      "📊 R² (Test): -0.0143, Adjusted R² (Test): -0.0186\n"
     ]
    }
   ],
   "source": [
    "### Set seed for reproducibility\n",
    "seed_value = 17\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "### Define Time-Series Cross-Validation (5 splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5, max_train_size=None)\n",
    "\n",
    "### Hyperparameters for Random Forest\n",
    "n_estimators_values = [50, 100, 200, 300]  # Number of trees to test\n",
    "max_depth_values = [5, 10, 20, None]  # Depth of trees\n",
    "\n",
    "\n",
    "### Function to compute Adjusted R²\n",
    "def adjusted_r2(r2, n, k):\n",
    "    if n <= k + 1:\n",
    "        return np.nan  # Avoid division by zero\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "### Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "### Perform Time-Series Cross-Validation\n",
    "for train_idx, test_idx in tscv.split(X1):\n",
    "    print(f\"Train size: {len(train_idx)}, Test size: {len(test_idx)}\")  # Debug check\n",
    "\n",
    "    ## Split dataset into train & test per fold (using `.loc[]` for safe indexing)\n",
    "    X1_train, X1_test = X1.loc[X1.index[train_idx]], X1.loc[X1.index[test_idx]]\n",
    "    y_train, y_test = y.loc[y.index[train_idx]], y.loc[y.index[test_idx]]\n",
    "\n",
    "    ## Train and evaluate for each combination of hyperparameters\n",
    "    for n_estimators in n_estimators_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            \n",
    "            # Define the Random Forest model\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=seed_value,\n",
    "                n_jobs=-1  # Use all available processors\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X1_train, y_train)\n",
    "\n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X1_train)\n",
    "            y_test_pred = model.predict(X1_test)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            mse = mean_squared_error(y_test, y_test_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "            # Compute Adjusted R²\n",
    "            n_train, k = X1_train.shape\n",
    "            n_test = X1_test.shape[0]\n",
    "            adj_r2_train = adjusted_r2(r2_train, n_train, k)\n",
    "            adj_r2_test = adjusted_r2(r2_test, n_test, k)\n",
    "\n",
    "            # Store results for this combination (append results for all folds)\n",
    "            if (n_estimators, max_depth) not in results:\n",
    "                results[(n_estimators, max_depth)] = []\n",
    "\n",
    "            results[(n_estimators, max_depth)].append({\n",
    "                \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2_test\": r2_test, \"R2_train\": r2_train,\n",
    "                \"Adj_R2_test\": adj_r2_test, \"Adj_R2_train\": adj_r2_train\n",
    "            })\n",
    "\n",
    "### Compute Average Performance Across Folds\n",
    "averaged_results = {\n",
    "    params: {metric: np.mean([fold[metric] for fold in folds])\n",
    "             for metric in folds[0].keys()}\n",
    "    for params, folds in results.items()\n",
    "}\n",
    "\n",
    "### Find the best hyperparameter combination (minimize MSE, maximize R²)\n",
    "best_params = min(averaged_results, key=lambda x: (averaged_results[x][\"MSE\"], -averaged_results[x][\"R2_test\"]))\n",
    "best_metrics = averaged_results[best_params]\n",
    "\n",
    "### Print optimal hyperparameters and their performance\n",
    "print(f\"🌲 Optimal Random Forest Parameters: n_estimators={best_params[0]}, max_depth={best_params[1]}\")\n",
    "print(f\"📊 Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"📊 Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"📊 Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"📊 R² (Train): {best_metrics['R2_train']:.4f}, Adjusted R² (Train): {best_metrics['Adj_R2_train']:.4f}\")\n",
    "print(f\"📊 R² (Test): {best_metrics['R2_test']:.4f}, Adjusted R² (Test): {best_metrics['Adj_R2_test']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Again"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1. Implement GroupKFold for Panel Data\n",
    "\n",
    "We need to group by \"Municipio\" when performing cross-validation.\n",
    "\n",
    "- Ensures that all data from one municipality is only in train or test (not both).\n",
    "- Respects time structure by sorting by \"AÑO\" first.\n",
    "- Prevents municipalities from leaking into both train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 8075, Test size: 2016\n",
      "Train size: 8075, Test size: 2016\n",
      "Train size: 8069, Test size: 2022\n",
      "Train size: 8076, Test size: 2015\n",
      "Train size: 8069, Test size: 2022\n"
     ]
    }
   ],
   "source": [
    "# Ensure y is a Pandas Series (not a NumPy array)\n",
    "if isinstance(y, np.ndarray):  \n",
    "    y = pd.Series(y, index=X1.index)  # Restore indexing\n",
    "\n",
    "\n",
    "\n",
    "from sklearn.model_selection import GroupKFold\n",
    "\n",
    "# Define the number of splits\n",
    "n_splits = 5  \n",
    "tscv = GroupKFold(n_splits=n_splits)\n",
    "\n",
    "# Extract groups (Municipio column as identifier)\n",
    "groups = X1[\"Municipio\"].values  \n",
    "\n",
    "for train_idx, test_idx in tscv.split(X1, y, groups=groups):\n",
    "    X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    print(f\"Train size: {len(train_idx)}, Test size: {len(test_idx)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2. Train a Quick Random Forest to Identify Important Features\n",
    "\n",
    "Before running the full model, we train a baseline Random Forest to rank feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "    Feature  Importance\n",
      "0  ICM-00-0    0.395368\n",
      "6  SOS-00-0    0.166915\n",
      "1  PCC-00-0    0.109436\n",
      "2  GPI-00-0    0.103235\n",
      "3  EIS-00-0    0.090039\n",
      "5  SEG-00-0    0.069330\n",
      "4  CTI-00-0    0.065677\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Define a baseline Random Forest model\n",
    "rf_baseline = RandomForestRegressor(\n",
    "    n_estimators=50,  # Small number of trees for quick evaluation\n",
    "    max_depth=10,     # Limit tree depth to avoid overfitting\n",
    "    random_state=17,\n",
    "    n_jobs=-1         # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Fit the model on the full dataset (ignoring \"AÑO\" and \"Municipio\" for now)\n",
    "X1_selected = X1.drop(columns=[\"Aglomeración\", \"SC Función ciudades\", \"Departamento\", \"Municipio\", \"Divipola\", \"AÑO\"])  # Remove non-predictive ID variables\n",
    "rf_baseline.fit(X1_selected, y)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X1_selected.columns,\n",
    "    \"Importance\": rf_baseline.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display the top 30 features\n",
    "print(feature_importance_df.head(30))\n",
    "#import ace_tools as tools\n",
    "#tools.display_dataframe_to_user(name=\"Top Feature Importance\", dataframe=feature_importance_df.head(3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.3. Features "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.39536799, 0.10943638, 0.10323458, 0.09003945, 0.06567688,\n",
       "       0.06932975, 0.16691497])"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Keep only the top 30 most important features\n",
    "#top_features = feature_importance_df[\"Feature\"].head(30).tolist()\n",
    "\n",
    "# Update X1 to only include the top features\n",
    "#X1 = X1[top_features]\n",
    "\n",
    "#print(f\"✅ Selected top {len(top_features)} features for the final model.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.4. Train the Final Random Forest Model\n",
    "\n",
    "Train Random Forest with Cross-Validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🌲 Best Random Forest Parameters: n_estimators=300, max_depth=None\n",
      "📊 Best MSE: 1285427.8743\n",
      "📊 Best RMSE: 1133.7549\n",
      "📊 Best MAE: 877.6061\n",
      "📊 R² (Train): 0.9542\n",
      "📊 R² (Test): 0.6917\n"
     ]
    }
   ],
   "source": [
    "# Define hyperparameters for tuning\n",
    "n_estimators_values = [50, 100, 200, 300]  # Number of trees\n",
    "max_depth_values = [5, 10, 20, None]       # Depth of trees\n",
    "\n",
    "# Store results\n",
    "results = {}\n",
    "\n",
    "# Perform Time-Series Cross-Validation\n",
    "from sklearn.model_selection import GroupKFold\n",
    "tscv = GroupKFold(n_splits=5)\n",
    "groups = X1.index  # Using index since Municipio was dropped\n",
    "\n",
    "for train_idx, test_idx in tscv.split(X1, y, groups=groups):\n",
    "    X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    # Train and evaluate for each combination of hyperparameters\n",
    "    for n_estimators in n_estimators_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            \n",
    "            # Define the Random Forest model\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=17,\n",
    "                n_jobs=-1  # Use all available processors\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X1_train, y_train)\n",
    "\n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X1_train)\n",
    "            y_test_pred = model.predict(X1_test)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "            mse = mean_squared_error(y_test, y_test_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "            # Store results\n",
    "            if (n_estimators, max_depth) not in results:\n",
    "                results[(n_estimators, max_depth)] = []\n",
    "\n",
    "            results[(n_estimators, max_depth)].append({\n",
    "                \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2_test\": r2_test, \"R2_train\": r2_train\n",
    "            })\n",
    "\n",
    "### Compute Average Performance Across Folds\n",
    "averaged_results = {\n",
    "    params: {metric: np.mean([fold[metric] for fold in folds])\n",
    "             for metric in folds[0].keys()}\n",
    "    for params, folds in results.items()\n",
    "}\n",
    "\n",
    "### Find the best hyperparameter combination\n",
    "best_params = min(averaged_results, key=lambda x: (averaged_results[x][\"MSE\"], -averaged_results[x][\"R2_test\"]))\n",
    "best_metrics = averaged_results[best_params]\n",
    "\n",
    "### Print optimal hyperparameters and performance\n",
    "print(f\"🌲 Best Random Forest Parameters: n_estimators={best_params[0]}, max_depth={best_params[1]}\")\n",
    "print(f\"📊 Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"📊 Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"📊 Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"📊 R² (Train): {best_metrics['R2_train']:.4f}\")\n",
    "print(f\"📊 R² (Test): {best_metrics['R2_test']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6837566,
     "sourceId": 10985952,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "MDS_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
