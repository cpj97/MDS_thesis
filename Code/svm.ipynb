{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement upgrade (from versions: none)\n",
      "ERROR: No matching distribution found for upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118\n",
      "Requirement already satisfied: torch in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from sympy->torch) (1.3.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING: Requirement 'thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl' looks like a filename, but the file does not exist\n",
      "ERROR: Invalid requirement: 'thundersvm==cu90': Expected end or semicolon (after name and no valid version specifier)\n",
      "    thundersvm==cu90\n",
      "              ^\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pandas matplotlib numpy scikit-learn ace_tools panelsplit shap upgrade jupyter ipywidgets \n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install thundersvm-cu90-0.2.0-py3-none-linux_x86_64.whl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "Please build the library first!",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mthundersvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SVC \n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mshap\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cpedr\\anaconda3\\envs\\nlp\\Lib\\site-packages\\thundersvm\\__init__.py:10\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;124;03m * Name        : __init__.py\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;124;03m * Author      : Locke <luojiahuan001@gmail.com>\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;124;03m * Version     : 0.0.1\u001b[39;00m\n\u001b[0;32m      7\u001b[0m \u001b[38;5;124;03m * Description :\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m      9\u001b[0m name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mthundersvm\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m---> 10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mthundersvm\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\cpedr\\anaconda3\\envs\\nlp\\Lib\\site-packages\\thundersvm\\thundersvm.py:52\u001b[0m\n\u001b[0;32m     50\u001b[0m         thundersvm \u001b[38;5;241m=\u001b[39m CDLL(lib_path)\n\u001b[0;32m     51\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m---> 52\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mFileNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mPlease build the library first!\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m     53\u001b[0m SVM_TYPE \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mc_svc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnu_svc\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mone_class\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepsilon_svr\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mnu_svr\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m     54\u001b[0m KERNEL_TYPE \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlinear\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpolynomial\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrbf\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124msigmoid\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mprecomputed\u001b[39m\u001b[38;5;124m'\u001b[39m]\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: Please build the library first!"
     ]
    }
   ],
   "source": [
    "from thundersvm import SVC \n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from panelsplit.cross_validation import PanelSplit\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import ParameterSampler\n",
    "from tqdm import tqdm\n",
    "\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define file names\n",
    "csv_filename = \"../final_df.csv\"\n",
    "zip_filename = \"../final_df.zip\"\n",
    "\n",
    "# Extract the CSV file from the ZIP\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zipf:\n",
    "    zipf.extract(csv_filename)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "final_df = pd.read_csv(\"final_df.csv\")\n",
    "\n",
    "# Delete the extracted CSV file after reading\n",
    "os.remove(\"final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "71\n"
     ]
    }
   ],
   "source": [
    "#Getting rid of redundant columns\n",
    "# X final\n",
    "X1 = final_df.drop(columns=['year', 'codmpio', 'cluster_kmeans', 'Departamento', 'Municipio', 'Region', 'pc_loss', 'f_loss', 'areaoficialhm2', 'gdp']) # GDP, area already out since the were already considered\n",
    "X1 = X1.drop(columns=['CV-01-15','CV-03-64', 'CV-03-26b', 'CV-01-11', 'CV-01-1', 'PCC-02-0', 'PCC-02-3', 'EIS-00-0', 'PCC-00-0', 'CTI-00-0', 'C-02-8t', 'ICM-00-0',\n",
    "                      'EIS-03-4', 'CTI-01-3', 'SEG-00-0', 'SEG-01-6', 'SOS-00-0', 'SOS-02-0', 'SOS-02-2', 'SOS-01-6', 'GPI-00-0', 'GPI-02-4', 'GPI-02-5', 'P-01-34-1',\n",
    "                      'P-01-46', 'P-01-25', 'CV-02-12e', 'CV-03-51', 'total_ac']) \n",
    "\n",
    "print(len(X1.columns))\n",
    "# Keep feature names\n",
    "original_feature_names = list(X1.columns) \n",
    "\n",
    "# y \n",
    "y = final_df['pc_loss']\n",
    "y = np.log1p(final_df['pc_loss'])\n",
    "\n",
    "# Normalize output\n",
    "scaler = StandardScaler()\n",
    "X1 = scaler.fit_transform(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cpedr\\anaconda3\\envs\\nlp\\Lib\\site-packages\\sklearn\\model_selection\\_search.py:320: UserWarning: The total space of parameters 48 is smaller than n_iter=100. Running 48 iterations. For exhaustive searches, use GridSearchCV.\n",
      "  warnings.warn(\n",
      "🔍 Searching Hyperparameters (Cross-Validated):   2%|▏         | 1/48 [02:02<1:36:04, 122.65s/it]"
     ]
    }
   ],
   "source": [
    "# Set seed for reproducibility\n",
    "seed_value = 17\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# 🔹 Define SVR Model Parameters Grid\n",
    "param_dist_svr = {\n",
    "    \"C\": [0.1, 1, 10, 100],            # Regularization parameter\n",
    "    \"epsilon\": [0.01, 0.1, 1],           # Epsilon in the epsilon-SVR model\n",
    "    \"kernel\": [\"rbf\", \"linear\"],         # Kernel type\n",
    "    \"gamma\": [\"scale\", \"auto\"]           # Kernel coefficient for 'rbf'\n",
    "}\n",
    "\n",
    "# 🔹 Define Time Series Split (PanelSplit for panel data)\n",
    "panel_split = PanelSplit(periods=final_df.year, n_splits=5)\n",
    "\n",
    "# 🔹 Store All Results for SVR\n",
    "cv_results_svr = []\n",
    "saved_models_svr = []\n",
    "\n",
    "# Compute total iterations across all folds (for progress tracking)\n",
    "total_iters = 100 * panel_split.n_splits  # Adjust if needed\n",
    "\n",
    "# Prepare Randomized Parameter Search for SVR\n",
    "param_list_svr = list(ParameterSampler(param_dist_svr, n_iter=100, random_state=seed_value))\n",
    "\n",
    "# Global progress bar across parameter sets\n",
    "with tqdm(total=len(param_list_svr), desc=\"🔍 Searching Hyperparameters (Cross-Validated)\") as pbar:\n",
    "    for params in param_list_svr:\n",
    "        fold_metrics = []\n",
    "        fold_models = []\n",
    "        \n",
    "        # Cross-validation using PanelSplit\n",
    "        for fold, (train_idx, test_idx) in enumerate(panel_split.split(X1)):\n",
    "            # Train-test split\n",
    "            X_train, X_test = X1[train_idx], X1[test_idx]\n",
    "            y_train, y_test = y[train_idx], y[test_idx]\n",
    "            \n",
    "            # 🔹 Preprocess: Standardize features (essential for SVM)\n",
    "            scaler = StandardScaler()\n",
    "            X_train_scaled = scaler.fit_transform(X_train)\n",
    "            X_test_scaled = scaler.transform(X_test)\n",
    "            \n",
    "            # 🔹 Define and train SVR model\n",
    "            svr_model = SVR(**params)\n",
    "            svr_model.fit(X_train_scaled, y_train)\n",
    "            \n",
    "            # 🔹 Predict\n",
    "            y_test_pred = svr_model.predict(X_test_scaled)\n",
    "            y_train_pred = svr_model.predict(X_train_scaled)\n",
    "            \n",
    "            # 🔹 Compute Metrics\n",
    "            mse = mean_squared_error(y_test, y_test_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "            \n",
    "            fold_metrics.append({\n",
    "                \"MSE\": mse,\n",
    "                \"RMSE\": rmse,\n",
    "                \"MAE\": mae,\n",
    "                \"R2_test\": r2_test,\n",
    "                \"R2_train\": r2_train\n",
    "            })\n",
    "            fold_models.append(svr_model)\n",
    "        \n",
    "        # 🔹 Average Metrics across folds\n",
    "        avg_metrics = {\n",
    "            \"MSE\": np.mean([m[\"MSE\"] for m in fold_metrics]),\n",
    "            \"RMSE\": np.mean([m[\"RMSE\"] for m in fold_metrics]),\n",
    "            \"MAE\": np.mean([m[\"MAE\"] for m in fold_metrics]),\n",
    "            \"R2_test\": np.mean([m[\"R2_test\"] for m in fold_metrics]),\n",
    "            \"R2_train\": np.mean([m[\"R2_train\"] for m in fold_metrics])\n",
    "        }\n",
    "        \n",
    "        saved_models_svr.append({\n",
    "            \"params\": params,\n",
    "            \"metrics\": avg_metrics,\n",
    "            \"models\": fold_models  # Optionally select the best model among folds\n",
    "        })\n",
    "        pbar.update(1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import cupy as cp\n",
    "from cuml.svm import SVR as cumlSVR\n",
    "from cuml.preprocessing import StandardScaler as cumlStandardScaler\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_value = 17\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Assuming final_df and PanelSplit are defined as in your original code:\n",
    "# For example, PanelSplit may look like:\n",
    "# panel_split = PanelSplit(periods=final_df.year, n_splits=5)\n",
    "panel_split = PanelSplit(periods=final_df.year, n_splits=5)\n",
    "\n",
    "# For demonstration, store metrics from each fold\n",
    "cv_results_gpu = []\n",
    "\n",
    "# Global progress bar for iterating folds (or you can wrap hyperparameter tuning later)\n",
    "with tqdm(total=panel_split.n_splits, desc=\"Processing Folds on GPU\") as pbar:\n",
    "    for fold, (train_idx, test_idx) in enumerate(panel_split.split(X1)):\n",
    "        # Split training and test sets using indices\n",
    "        X_train, X_test = X1[train_idx], X1[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        # Convert data to CuPy arrays for GPU processing\n",
    "        X_train_cp = cp.asarray(X_train)\n",
    "        X_test_cp = cp.asarray(X_test)\n",
    "        y_train_cp = cp.asarray(y_train)\n",
    "\n",
    "        # Standardize features using cuML's StandardScaler\n",
    "        scaler = cumlStandardScaler()\n",
    "        X_train_scaled = scaler.fit_transform(X_train_cp)\n",
    "        X_test_scaled = scaler.transform(X_test_cp)\n",
    "\n",
    "        # Define and train the GPU-enabled SVR model\n",
    "        # (Here we use a basic kernel and hyperparameters; these can be tuned later)\n",
    "        svr_gpu = cumlSVR(kernel=\"rbf\", C=1.0, epsilon=0.1)\n",
    "        svr_gpu.fit(X_train_scaled, y_train_cp)\n",
    "\n",
    "        # Predict on training and test sets\n",
    "        y_train_pred_cp = svr_gpu.predict(X_train_scaled)\n",
    "        y_test_pred_cp = svr_gpu.predict(X_test_scaled)\n",
    "\n",
    "        # Convert predictions from CuPy to NumPy for metric evaluation\n",
    "        y_train_pred = cp.asnumpy(y_train_pred_cp)\n",
    "        y_test_pred = cp.asnumpy(y_test_pred_cp)\n",
    "\n",
    "        # Evaluate predictions using scikit-learn metrics\n",
    "        mse = mean_squared_error(y_test, y_test_pred)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test, y_test_pred)\n",
    "        r2_test = r2_score(y_test, y_test_pred)\n",
    "        r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "        cv_results_gpu.append({\n",
    "            \"fold\": fold,\n",
    "            \"MSE\": mse,\n",
    "            \"RMSE\": rmse,\n",
    "            \"MAE\": mae,\n",
    "            \"R2_test\": r2_test,\n",
    "            \"R2_train\": r2_train\n",
    "        })\n",
    "        \n",
    "        pbar.update(1)\n",
    "\n",
    "# Print or log the cross-validation results\n",
    "print(\"GPU-accelerated SVR cross-validation results:\")\n",
    "for res in cv_results_gpu:\n",
    "    print(res)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
