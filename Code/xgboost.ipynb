{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBoost"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ERROR: Could not find a version that satisfies the requirement upgrade (from versions: none)\n",
      "ERROR: No matching distribution found for upgrade\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://download.pytorch.org/whl/cu118Note: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "Requirement already satisfied: torch in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: torchvision in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (0.19.1)\n",
      "Requirement already satisfied: torchaudio in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (2.4.1+cu118)\n",
      "Requirement already satisfied: filelock in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.15.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (4.12.2)\n",
      "Requirement already satisfied: sympy in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (1.13.2)\n",
      "Requirement already satisfied: networkx in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.3)\n",
      "Requirement already satisfied: jinja2 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (2024.6.1)\n",
      "Requirement already satisfied: setuptools in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torch) (74.1.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchvision) (1.26.4)\n",
      "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from torchvision) (10.4.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from jinja2->torch) (2.1.5)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in c:\\users\\cpedr\\anaconda3\\envs\\nlp\\lib\\site-packages (from sympy->torch) (1.3.0)\n"
     ]
    }
   ],
   "source": [
    "%pip install -q pandas matplotlib numpy scikit-learn ace_tools panelsplit shap upgrade jupyter ipywidgets xgboost\n",
    "%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\cpedr\\anaconda3\\envs\\nlp\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import xgboost as xgb\n",
    "import torch\n",
    "import shap\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from panelsplit.cross_validation import PanelSplit\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '../final_df.zip'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 6\u001b[0m\n\u001b[0;32m      3\u001b[0m zip_filename \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m../final_df.zip\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m# Extract the CSV file from the ZIP\u001b[39;00m\n\u001b[1;32m----> 6\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mzipfile\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mZipFile\u001b[49m\u001b[43m(\u001b[49m\u001b[43mzip_filename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mr\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m zipf:\n\u001b[0;32m      7\u001b[0m     zipf\u001b[38;5;241m.\u001b[39mextract(csv_filename)\n\u001b[0;32m      9\u001b[0m \u001b[38;5;66;03m# Read the CSV file into a DataFrame\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\cpedr\\anaconda3\\envs\\nlp\\Lib\\zipfile\\__init__.py:1331\u001b[0m, in \u001b[0;36mZipFile.__init__\u001b[1;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps, metadata_encoding)\u001b[0m\n\u001b[0;32m   1329\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m   1330\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m-> 1331\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfp \u001b[38;5;241m=\u001b[39m \u001b[43mio\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilemode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1332\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mOSError\u001b[39;00m:\n\u001b[0;32m   1333\u001b[0m         \u001b[38;5;28;01mif\u001b[39;00m filemode \u001b[38;5;129;01min\u001b[39;00m modeDict:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../final_df.zip'"
     ]
    }
   ],
   "source": [
    "# Define file names\n",
    "csv_filename = \"../final_df.csv\"\n",
    "zip_filename = \"../final_df.zip\"\n",
    "\n",
    "# Extract the CSV file from the ZIP\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zipf:\n",
    "    zipf.extract(csv_filename)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "final_df = pd.read_csv(\"final_df.csv\")\n",
    "\n",
    "# Delete the extracted CSV file after reading\n",
    "os.remove(\"final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data after 2007\n",
    "final_df = final_df[final_df['year'] > 2006]\n",
    "# y \n",
    "y = final_df['tc_loss']\n",
    "\n",
    "# Normalize output\n",
    "scaler = StandardScaler()\n",
    "y = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Get rid of columns that start with 'subnational1_' and 'cluster_' in train and test   \n",
    "#X1 = final_df.loc[:,~final_df.columns.str.startswith('g')]\n",
    "X1 = final_df.loc[:,~final_df.columns.str.startswith('subnational1_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('cluster_')]\n",
    "\n",
    "\n",
    "# Get rid of all disaggregated columns\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('ac_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('as_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('p_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('r_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('nuf_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('vrf_')]\n",
    "\n",
    "# X final\n",
    "X1 = X1.drop(columns=['year', 'tc_loss', 'tc_loss_area', 'codmpio'])\n",
    "\n",
    "# Keep feature names\n",
    "original_feature_names = list(X1.columns) \n",
    "\n",
    "# Create polynomial interaction terms (degree=2, only interactions, no bias term)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_interactions = poly.fit_transform(X1)\n",
    "\n",
    "# Get the feature names including interactions\n",
    "interaction_feature_names = poly.get_feature_names_out(original_feature_names)\n",
    "\n",
    "# Compute standard deviation of each feature\n",
    "stds = X1.std(axis=0)\n",
    "\n",
    "# Find columns where std = 0\n",
    "zero_variance_features = np.where(stds == 0)[0]\n",
    "\n",
    "if len(zero_variance_features) > 0:\n",
    "    print(f\"âš ï¸ Removing {len(zero_variance_features)} features with zero variance.\")\n",
    "    print(f\"ğŸ” Removed feature indices: {zero_variance_features}\")\n",
    "\n",
    "    # If feature names are available, print them\n",
    "    if isinstance(X1, pd.DataFrame):  # If X is a DataFrame\n",
    "        removed_feature_names = X1.columns[zero_variance_features]\n",
    "        print(f\"ğŸ“Œ Removed feature names: {list(removed_feature_names)}\")\n",
    "\n",
    "    X1 = np.delete(X1, zero_variance_features, axis=1)  # Remove constant columns\n",
    "\n",
    "\n",
    "# Normalize features\n",
    "X1 = scaler.fit_transform(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "# ğŸ”¹ Set Device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "# ğŸ”¹ Define XGBoost Model Parameters\n",
    "xgb_params = {\n",
    "    \"objective\": \"reg:squarederror\",\n",
    "    \"eval_metric\": \"rmse\",\n",
    "    \"max_depth\": 6,\n",
    "    \"learning_rate\": 0.05,\n",
    "    \"n_estimators\": 1000,\n",
    "    \"subsample\": 0.8,\n",
    "    \"colsample_bytree\": 0.8,\n",
    "    \"alpha\": 0.01,  # L1 Regularization\n",
    "    \"lambda\": 0.1,  # L2 Regularization\n",
    "    \"device\": \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "}\n",
    "\n",
    "panel_split = PanelSplit(periods=final_df.year, n_splits=5)\n",
    "\n",
    "# ğŸ”¹ Store results for cross-validation\n",
    "cv_results = []\n",
    "\n",
    "for train_idx, test_idx in panel_split.split(X1):\n",
    "\n",
    "    # Create interaction terms (degree=2 means pairwise interactions)\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    X_interactions = poly.fit_transform(X1)\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test = X1[train_idx], X1[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Convert to PyTorch tensors (for consistency)\n",
    "    X_train_tensor = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "    X_test_tensor = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "    y_train_tensor = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    y_test_tensor = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    # ğŸ”¹ Train XGBoost Model\n",
    "    xgb_model = xgb.XGBRegressor(**xgb_params)\n",
    "    xgb_model.fit(X_train_tensor.cpu().numpy(), y_train_tensor.cpu().numpy(),\n",
    "                  eval_set=[(X_test_tensor.cpu().numpy(), y_test_tensor.cpu().numpy())],\n",
    "                  early_stopping_rounds=50, verbose=False)\n",
    "\n",
    "    # ğŸ”¹ Get Predictions\n",
    "    y_train_pred = xgb_model.predict(X_train_tensor.cpu().numpy())\n",
    "    y_test_pred = xgb_model.predict(X_test_tensor.cpu().numpy())\n",
    "\n",
    "    # ğŸ”¹ Compute Performance Metrics\n",
    "    mse = mean_squared_error(y_test_tensor.cpu().numpy(), y_test_pred)\n",
    "    rmse = np.sqrt(mse)\n",
    "    r2_test = r2_score(y_test_tensor.cpu().numpy(), y_test_pred)\n",
    "    r2_train = r2_score(y_train_tensor.cpu().numpy(), y_train_pred)\n",
    "\n",
    "    # ğŸ”¹ Store Results\n",
    "    cv_results.append({\"MSE\": mse, \"RMSE\": rmse, \"R2_train\": r2_train, \"R2_test\": r2_test})\n",
    "\n",
    "# ğŸ”¹ Compute Averages Across CV Splits\n",
    "avg_results = {metric: np.mean([fold[metric] for fold in cv_results]) for metric in cv_results[0].keys()}\n",
    "\n",
    "# ğŸ”¹ Print Results\n",
    "print(f\"âœ… XGBoost with TimeSeriesSplit Results:\")\n",
    "print(f\"ğŸ“Š Avg MSE: {avg_results['MSE']:.4f}\")\n",
    "print(f\"ğŸ“Š Avg RMSE: {avg_results['RMSE']:.4f}\")\n",
    "print(f\"ğŸ“Š Avg RÂ² (Train): {avg_results['R2_train']:.4f}\")\n",
    "print(f\"ğŸ“Š Avg RÂ² (Test): {avg_results['R2_test']:.4f}\")\n",
    "\n",
    "# ğŸ”¹ Compute SHAP Values for Feature Importance\n",
    "explainer = shap.Explainer(xgb_model)\n",
    "shap_values = explainer(X_test_tensor.cpu().numpy())\n",
    "\n",
    "# ğŸ”¹ Plot SHAP Feature Importance\n",
    "shap.plots.bar(shap_values)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nlp",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
