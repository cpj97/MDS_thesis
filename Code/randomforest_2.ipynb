{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0.Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:11.635852Z",
     "iopub.status.busy": "2025-03-12T09:07:11.635460Z",
     "iopub.status.idle": "2025-03-12T09:07:19.489740Z",
     "shell.execute_reply": "2025-03-12T09:07:19.488687Z",
     "shell.execute_reply.started": "2025-03-12T09:07:11.635823Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting shap\n",
      "  Downloading shap-0.47.0.tar.gz (2.5 MB)\n",
      "     ---------------------------------------- 0.0/2.5 MB ? eta -:--:--\n",
      "     ---------------- ----------------------- 1.0/2.5 MB 18.7 MB/s eta 0:00:01\n",
      "     ---------------- ----------------------- 1.0/2.5 MB 18.7 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.6/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.6/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     ------------------------- -------------- 1.6/2.5 MB 2.8 MB/s eta 0:00:01\n",
      "     --------------------------------- ------ 2.1/2.5 MB 1.6 MB/s eta 0:00:01\n",
      "     ---------------------------------------- 2.5/2.5 MB 1.6 MB/s eta 0:00:00\n",
      "  Installing build dependencies: started\n",
      "  Installing build dependencies: finished with status 'done'\n",
      "  Getting requirements to build wheel: started\n",
      "  Getting requirements to build wheel: finished with status 'done'\n",
      "  Preparing metadata (pyproject.toml): started\n",
      "  Preparing metadata (pyproject.toml): finished with status 'done'\n",
      "Requirement already satisfied: numpy in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: scipy in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from shap) (1.15.2)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from shap) (1.6.1)\n",
      "Requirement already satisfied: pandas in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from shap) (2.2.3)\n",
      "Requirement already satisfied: tqdm>=4.27.0 in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from shap) (4.67.1)\n",
      "Requirement already satisfied: packaging>20.9 in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from shap) (24.2)\n",
      "Collecting slicer==0.0.8 (from shap)\n",
      "  Downloading slicer-0.0.8-py3-none-any.whl.metadata (4.0 kB)\n",
      "Collecting numba>=0.54 (from shap)\n",
      "  Downloading numba-0.61.0-cp313-cp313-win_amd64.whl.metadata (2.8 kB)\n",
      "Collecting cloudpickle (from shap)\n",
      "  Downloading cloudpickle-3.1.1-py3-none-any.whl.metadata (7.1 kB)\n",
      "Requirement already satisfied: typing-extensions in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from shap) (4.12.2)\n",
      "Collecting llvmlite<0.45,>=0.44.0dev0 (from numba>=0.54->shap)\n",
      "  Downloading llvmlite-0.44.0-cp313-cp313-win_amd64.whl.metadata (5.0 kB)\n",
      "Collecting numpy (from shap)\n",
      "  Downloading numpy-2.1.3-cp313-cp313-win_amd64.whl.metadata (60 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from tqdm>=4.27.0->shap) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from pandas->shap) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from pandas->shap) (2025.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from scikit-learn->shap) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=3.1.0 in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from scikit-learn->shap) (3.5.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\mmier\\anaconda3\\envs\\mds_thesis\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->shap) (1.17.0)\n",
      "Downloading slicer-0.0.8-py3-none-any.whl (15 kB)\n",
      "Downloading numba-0.61.0-cp313-cp313-win_amd64.whl (2.8 MB)\n",
      "   ---------------------------------------- 0.0/2.8 MB ? eta -:--:--\n",
      "   -------------- ------------------------- 1.0/2.8 MB 13.8 MB/s eta 0:00:01\n",
      "   -------------- ------------------------- 1.0/2.8 MB 13.8 MB/s eta 0:00:01\n",
      "   ------------------ --------------------- 1.3/2.8 MB 2.4 MB/s eta 0:00:01\n",
      "   ---------------------- ----------------- 1.6/2.8 MB 1.6 MB/s eta 0:00:01\n",
      "   ------------------------- -------------- 1.8/2.8 MB 1.8 MB/s eta 0:00:01\n",
      "   ----------------------------- ---------- 2.1/2.8 MB 1.5 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.4/2.8 MB 1.6 MB/s eta 0:00:01\n",
      "   --------------------------------- ------ 2.4/2.8 MB 1.6 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 2.8/2.8 MB 1.4 MB/s eta 0:00:00\n",
      "Downloading numpy-2.1.3-cp313-cp313-win_amd64.whl (12.6 MB)\n",
      "   ---------------------------------------- 0.0/12.6 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.3/12.6 MB ? eta -:--:--\n",
      "   - -------------------------------------- 0.5/12.6 MB 1.4 MB/s eta 0:00:09\n",
      "   -- ------------------------------------- 0.8/12.6 MB 1.3 MB/s eta 0:00:10\n",
      "   --- ------------------------------------ 1.0/12.6 MB 1.3 MB/s eta 0:00:10\n",
      "   ---- ----------------------------------- 1.3/12.6 MB 1.3 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.6/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ----- ---------------------------------- 1.8/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ------ --------------------------------- 2.1/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.4/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.4/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   ------- -------------------------------- 2.4/12.6 MB 1.2 MB/s eta 0:00:09\n",
      "   -------- ------------------------------- 2.6/12.6 MB 999.6 kB/s eta 0:00:10\n",
      "   --------- ------------------------------ 2.9/12.6 MB 1.0 MB/s eta 0:00:10\n",
      "   ---------- ----------------------------- 3.4/12.6 MB 1.1 MB/s eta 0:00:09\n",
      "   ----------- ---------------------------- 3.7/12.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------ --------------------------- 3.9/12.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.2/12.6 MB 1.1 MB/s eta 0:00:08\n",
      "   ------------- -------------------------- 4.2/12.6 MB 1.1 MB/s eta 0:00:08\n",
      "   -------------- ------------------------- 4.5/12.6 MB 1.1 MB/s eta 0:00:08\n",
      "   --------------- ------------------------ 4.7/12.6 MB 1.1 MB/s eta 0:00:07\n",
      "   --------------- ------------------------ 5.0/12.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ---------------- ----------------------- 5.2/12.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ----------------- ---------------------- 5.5/12.6 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------ --------------------- 5.8/12.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.0/12.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------- -------------------- 6.0/12.6 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------- ------------------- 6.6/12.6 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.1 MB/s eta 0:00:06\n",
      "   --------------------- ------------------ 6.8/12.6 MB 1.1 MB/s eta 0:00:06\n",
      "   ---------------------- ----------------- 7.1/12.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ----------------------- ---------------- 7.3/12.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------ --------------- 7.6/12.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 7.9/12.6 MB 1.1 MB/s eta 0:00:05\n",
      "   ------------------------- -------------- 8.1/12.6 MB 1.1 MB/s eta 0:00:04\n",
      "   -------------------------- ------------- 8.4/12.6 MB 1.1 MB/s eta 0:00:04\n",
      "   --------------------------- ------------ 8.7/12.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------- ----------- 8.9/12.6 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------- ---------- 9.2/12.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.4/12.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------ --------- 9.7/12.6 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------- -------- 10.0/12.6 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.2/12.6 MB 1.1 MB/s eta 0:00:03\n",
      "   -------------------------------- ------- 10.2/12.6 MB 1.1 MB/s eta 0:00:03\n",
      "   --------------------------------- ------ 10.5/12.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ---------------------------------- ----- 10.7/12.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.0/12.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ----------------------------------- ---- 11.3/12.6 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------ --- 11.5/12.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ------------------------------------- -- 11.8/12.6 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 12.1/12.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  12.3/12.6 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 12.6/12.6 MB 1.1 MB/s eta 0:00:00\n",
      "Downloading cloudpickle-3.1.1-py3-none-any.whl (20 kB)\n",
      "Downloading llvmlite-0.44.0-cp313-cp313-win_amd64.whl (30.3 MB)\n",
      "   ---------------------------------------- 0.0/30.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/30.3 MB ? eta -:--:--\n",
      "   ---------------------------------------- 0.3/30.3 MB ? eta -:--:--\n",
      "    --------------------------------------- 0.5/30.3 MB 589.8 kB/s eta 0:00:51\n",
      "   - -------------------------------------- 0.8/30.3 MB 914.5 kB/s eta 0:00:33\n",
      "   - -------------------------------------- 1.0/30.3 MB 1.1 MB/s eta 0:00:28\n",
      "   - -------------------------------------- 1.3/30.3 MB 1.1 MB/s eta 0:00:26\n",
      "   -- ------------------------------------- 1.6/30.3 MB 977.9 kB/s eta 0:00:30\n",
      "   -- ------------------------------------- 1.8/30.3 MB 1.0 MB/s eta 0:00:28\n",
      "   -- ------------------------------------- 2.1/30.3 MB 1.1 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 2.4/30.3 MB 1.1 MB/s eta 0:00:27\n",
      "   --- ------------------------------------ 2.6/30.3 MB 1.1 MB/s eta 0:00:26\n",
      "   --- ------------------------------------ 2.9/30.3 MB 1.1 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 3.1/30.3 MB 1.1 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 3.4/30.3 MB 1.1 MB/s eta 0:00:25\n",
      "   ---- ----------------------------------- 3.4/30.3 MB 1.1 MB/s eta 0:00:25\n",
      "   ----- ---------------------------------- 3.9/30.3 MB 1.1 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 3.9/30.3 MB 1.1 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 4.2/30.3 MB 1.1 MB/s eta 0:00:24\n",
      "   ----- ---------------------------------- 4.5/30.3 MB 1.1 MB/s eta 0:00:24\n",
      "   ------ --------------------------------- 4.7/30.3 MB 1.1 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 5.0/30.3 MB 1.1 MB/s eta 0:00:23\n",
      "   ------ --------------------------------- 5.2/30.3 MB 1.1 MB/s eta 0:00:23\n",
      "   ------- -------------------------------- 5.5/30.3 MB 1.1 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 5.8/30.3 MB 1.1 MB/s eta 0:00:22\n",
      "   ------- -------------------------------- 6.0/30.3 MB 1.1 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 6.3/30.3 MB 1.1 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 6.3/30.3 MB 1.1 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 6.6/30.3 MB 1.1 MB/s eta 0:00:22\n",
      "   -------- ------------------------------- 6.8/30.3 MB 1.1 MB/s eta 0:00:22\n",
      "   --------- ------------------------------ 7.1/30.3 MB 1.1 MB/s eta 0:00:21\n",
      "   --------- ------------------------------ 7.3/30.3 MB 1.1 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 7.6/30.3 MB 1.1 MB/s eta 0:00:21\n",
      "   ---------- ----------------------------- 7.9/30.3 MB 1.1 MB/s eta 0:00:20\n",
      "   ---------- ----------------------------- 7.9/30.3 MB 1.1 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 8.4/30.3 MB 1.1 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 8.7/30.3 MB 1.1 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 8.7/30.3 MB 1.1 MB/s eta 0:00:20\n",
      "   ----------- ---------------------------- 8.9/30.3 MB 1.1 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 9.2/30.3 MB 1.1 MB/s eta 0:00:20\n",
      "   ------------ --------------------------- 9.4/30.3 MB 1.1 MB/s eta 0:00:19\n",
      "   ------------ --------------------------- 9.7/30.3 MB 1.1 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 10.0/30.3 MB 1.1 MB/s eta 0:00:19\n",
      "   ------------- -------------------------- 10.2/30.3 MB 1.1 MB/s eta 0:00:18\n",
      "   ------------- -------------------------- 10.2/30.3 MB 1.1 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 10.7/30.3 MB 1.1 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 11.0/30.3 MB 1.1 MB/s eta 0:00:18\n",
      "   -------------- ------------------------- 11.3/30.3 MB 1.1 MB/s eta 0:00:18\n",
      "   --------------- ------------------------ 11.5/30.3 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 11.8/30.3 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 12.1/30.3 MB 1.1 MB/s eta 0:00:17\n",
      "   --------------- ------------------------ 12.1/30.3 MB 1.1 MB/s eta 0:00:17\n",
      "   ---------------- ----------------------- 12.6/30.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 12.6/30.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 12.8/30.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ---------------- ----------------------- 12.8/30.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 13.4/30.3 MB 1.1 MB/s eta 0:00:16\n",
      "   ----------------- ---------------------- 13.6/30.3 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 13.9/30.3 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------ --------------------- 14.2/30.3 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 14.4/30.3 MB 1.1 MB/s eta 0:00:15\n",
      "   ------------------- -------------------- 14.7/30.3 MB 1.1 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 14.7/30.3 MB 1.1 MB/s eta 0:00:14\n",
      "   ------------------- -------------------- 14.9/30.3 MB 1.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 15.2/30.3 MB 1.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 15.5/30.3 MB 1.1 MB/s eta 0:00:14\n",
      "   -------------------- ------------------- 15.7/30.3 MB 1.1 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 16.0/30.3 MB 1.1 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 16.0/30.3 MB 1.1 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 16.3/30.3 MB 1.1 MB/s eta 0:00:13\n",
      "   --------------------- ------------------ 16.5/30.3 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 16.8/30.3 MB 1.1 MB/s eta 0:00:13\n",
      "   ---------------------- ----------------- 17.0/30.3 MB 1.1 MB/s eta 0:00:12\n",
      "   ---------------------- ----------------- 17.3/30.3 MB 1.1 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 17.6/30.3 MB 1.1 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 17.8/30.3 MB 1.1 MB/s eta 0:00:12\n",
      "   ----------------------- ---------------- 18.1/30.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 18.4/30.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 18.6/30.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ------------------------ --------------- 18.9/30.3 MB 1.1 MB/s eta 0:00:11\n",
      "   ------------------------- -------------- 19.1/30.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 19.1/30.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 19.4/30.3 MB 1.1 MB/s eta 0:00:10\n",
      "   ------------------------- -------------- 19.7/30.3 MB 1.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 19.9/30.3 MB 1.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 20.2/30.3 MB 1.1 MB/s eta 0:00:10\n",
      "   -------------------------- ------------- 20.4/30.3 MB 1.1 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 20.7/30.3 MB 1.1 MB/s eta 0:00:09\n",
      "   --------------------------- ------------ 21.0/30.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 21.2/30.3 MB 1.1 MB/s eta 0:00:09\n",
      "   ---------------------------- ----------- 21.5/30.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ---------------------------- ----------- 21.8/30.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 22.0/30.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 22.3/30.3 MB 1.1 MB/s eta 0:00:08\n",
      "   ----------------------------- ---------- 22.5/30.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 22.8/30.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 23.1/30.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------ --------- 23.3/30.3 MB 1.1 MB/s eta 0:00:07\n",
      "   ------------------------------- -------- 23.6/30.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 23.9/30.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 23.9/30.3 MB 1.1 MB/s eta 0:00:06\n",
      "   ------------------------------- -------- 24.1/30.3 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 24.4/30.3 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 24.6/30.3 MB 1.1 MB/s eta 0:00:06\n",
      "   -------------------------------- ------- 24.9/30.3 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 25.2/30.3 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 25.4/30.3 MB 1.1 MB/s eta 0:00:05\n",
      "   --------------------------------- ------ 25.7/30.3 MB 1.1 MB/s eta 0:00:05\n",
      "   ---------------------------------- ----- 26.0/30.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 26.2/30.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ---------------------------------- ----- 26.5/30.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 26.7/30.3 MB 1.1 MB/s eta 0:00:04\n",
      "   ----------------------------------- ---- 27.0/30.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 27.3/30.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ----------------------------------- ---- 27.3/30.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 27.5/30.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 28.0/30.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------ --- 28.0/30.3 MB 1.1 MB/s eta 0:00:03\n",
      "   ------------------------------------- -- 28.3/30.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 28.3/30.3 MB 1.1 MB/s eta 0:00:02\n",
      "   ------------------------------------- -- 28.6/30.3 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 28.8/30.3 MB 1.1 MB/s eta 0:00:02\n",
      "   -------------------------------------- - 29.4/30.3 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 29.4/30.3 MB 1.1 MB/s eta 0:00:01\n",
      "   -------------------------------------- - 29.4/30.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  29.6/30.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------  30.1/30.3 MB 1.1 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 30.3/30.3 MB 1.1 MB/s eta 0:00:00\n",
      "Building wheels for collected packages: shap\n",
      "  Building wheel for shap (pyproject.toml): started\n",
      "  Building wheel for shap (pyproject.toml): finished with status 'done'\n",
      "  Created wheel for shap: filename=shap-0.47.0-cp313-cp313-win_amd64.whl size=528512 sha256=baeb7fb4ba36bc83ecfebe045c7d3a78294b8c2caae184041c76a4fa7729a20c\n",
      "  Stored in directory: c:\\users\\mmier\\appdata\\local\\pip\\cache\\wheels\\54\\cf\\88\\6775a629a23a6a3b3a5261596fa02e27021828783fb7aa6fd4\n",
      "Successfully built shap\n",
      "Installing collected packages: slicer, numpy, llvmlite, cloudpickle, numba, shap\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "Successfully installed cloudpickle-3.1.1 llvmlite-0.44.0 numba-0.61.0 numpy-2.1.3 shap-0.47.0 slicer-0.0.8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "#%pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "%pip install shap"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:19.491696Z",
     "iopub.status.busy": "2025-03-12T09:07:19.491359Z",
     "iopub.status.idle": "2025-03-12T09:07:19.497614Z",
     "shell.execute_reply": "2025-03-12T09:07:19.496644Z",
     "shell.execute_reply.started": "2025-03-12T09:07:19.491669Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "\n",
    "# from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import random\n",
    "\n",
    "import zipfile\n",
    "import os\n",
    "\n",
    "from panelsplit.cross_validation import PanelSplit\n",
    "\n",
    "#import shap\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import Lasso"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:19.499458Z",
     "iopub.status.busy": "2025-03-12T09:07:19.499180Z",
     "iopub.status.idle": "2025-03-12T09:07:19.514457Z",
     "shell.execute_reply": "2025-03-12T09:07:19.513437Z",
     "shell.execute_reply.started": "2025-03-12T09:07:19.499433Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Define device\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")  # Check if GPU or CPU is being used"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Load Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### VS Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import train\n",
    "#train = pd.read_csv('../Data/train_test/train_df.csv')\n",
    "train = pd.read_csv('Data/train_test/train_df.csv')\n",
    "\n",
    "# Import test\n",
    "#test = pd.read_csv('../Data/train_test/test_df.csv')\n",
    "test = pd.read_csv('Data/train_test/test_df.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:19.516215Z",
     "iopub.status.busy": "2025-03-12T09:07:19.515863Z",
     "iopub.status.idle": "2025-03-12T09:07:23.923962Z",
     "shell.execute_reply": "2025-03-12T09:07:23.923025Z",
     "shell.execute_reply.started": "2025-03-12T09:07:19.516183Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: '/kaggle/input/final-ds'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mFileNotFoundError\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 8\u001b[39m\n\u001b[32m      5\u001b[39m ds_path = \u001b[33m\"\u001b[39m\u001b[33m/kaggle/input/final-ds\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m      7\u001b[39m \u001b[38;5;66;03m# Create a list with the files in the dataset (dataframes)\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m8\u001b[39m ds_files = \u001b[43mos\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mds_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;66;03m#list available files in the dataset\u001b[39;00m\n\u001b[32m     10\u001b[39m \u001b[38;5;66;03m# Load each file into a dictionary (assuming all files are csv)\u001b[39;00m\n\u001b[32m     11\u001b[39m ds = {file: pd.read_csv(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mds_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m file \u001b[38;5;129;01min\u001b[39;00m ds_files}\n",
      "\u001b[31mFileNotFoundError\u001b[39m: [WinError 3] The system cannot find the path specified: '/kaggle/input/final-ds'"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# Define dataset path\n",
    "ds_path = \"/kaggle/input/final-ds\"\n",
    "\n",
    "# Create a list with the files in the dataset (dataframes)\n",
    "ds_files = os.listdir(ds_path) #list available files in the dataset\n",
    "\n",
    "# Load each file into a dictionary (assuming all files are csv)\n",
    "ds = {file: pd.read_csv(f\"{ds_path}/{file}\") for file in ds_files}\n",
    "\n",
    "# Create the specific dataframe\n",
    "final_df = ds[ds_files[0]] #first csv file\n",
    "final_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:23.925137Z",
     "iopub.status.busy": "2025-03-12T09:07:23.924870Z",
     "iopub.status.idle": "2025-03-12T09:07:24.253388Z",
     "shell.execute_reply": "2025-03-12T09:07:24.252429Z",
     "shell.execute_reply.started": "2025-03-12T09:07:23.925116Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "\n",
    "# Data after 2007\n",
    "final_df = final_df[final_df['year'] > 2006]\n",
    "# y \n",
    "y = final_df['tc_loss']\n",
    "\n",
    "# Normalize output\n",
    "scaler = StandardScaler()\n",
    "y = scaler.fit_transform(y.values.reshape(-1, 1))\n",
    "\n",
    "# Get rid of columns that start with 'subnational1_' and 'cluster_' in train and test   \n",
    "#X1 = final_df.loc[:,~final_df.columns.str.startswith('g')]\n",
    "X1 = final_df.loc[:,~final_df.columns.str.startswith('subnational1_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('cluster_')]\n",
    "\n",
    "\n",
    "# Get rid of all disaggregated columns\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('ac_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('as_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('p_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('r_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('nuf_')]\n",
    "X1 = X1.loc[:,~X1.columns.str.startswith('vrf_')]\n",
    "\n",
    "# X final\n",
    "X1 = X1.drop(columns=['year', 'tc_loss', 'tc_loss_area', 'codmpio'])\n",
    "\n",
    "# Keep feature names\n",
    "original_feature_names = list(X1.columns) \n",
    "\n",
    "# Create polynomial interaction terms (degree=2, only interactions, no bias term)\n",
    "poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "X_interactions = poly.fit_transform(X1)\n",
    "\n",
    "# Get the feature names including interactions\n",
    "interaction_feature_names = poly.get_feature_names_out(original_feature_names)\n",
    "\n",
    "# Compute standard deviation of each feature\n",
    "stds = X1.std(axis=0)\n",
    "\n",
    "# Find columns where std = 0\n",
    "zero_variance_features = np.where(stds == 0)[0]\n",
    "\n",
    "if len(zero_variance_features) > 0:\n",
    "    print(f\"⚠️ Removing {len(zero_variance_features)} features with zero variance.\")\n",
    "    print(f\"🔍 Removed feature indices: {zero_variance_features}\")\n",
    "\n",
    "    # If feature names are available, print them\n",
    "    if isinstance(X1, pd.DataFrame):  # If X is a DataFrame\n",
    "        removed_feature_names = X1.columns[zero_variance_features]\n",
    "        print(f\"📌 Removed feature names: {list(removed_feature_names)}\")\n",
    "\n",
    "    X1 = np.delete(X1, zero_variance_features, axis=1)  # Remove constant columns\n",
    "\n",
    "\n",
    "# Normalize features\n",
    "X1 = scaler.fit_transform(X1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:24.254575Z",
     "iopub.status.busy": "2025-03-12T09:07:24.254282Z",
     "iopub.status.idle": "2025-03-12T09:07:24.259770Z",
     "shell.execute_reply": "2025-03-12T09:07:24.258928Z",
     "shell.execute_reply.started": "2025-03-12T09:07:24.254533Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "class LassoRegression(nn.Module):\n",
    "    def __init__(self, input_dim, l1_lambda=0.01):\n",
    "        super(LassoRegression, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, 1)\n",
    "        self.l1_lambda = l1_lambda  # Regularization strength\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.linear(x)\n",
    "\n",
    "    def l1_regularization_loss(self):\n",
    "        return self.l1_lambda * torch.norm(self.linear.weight, p=1)  # L1 Regularization (Lasso)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:24.260944Z",
     "iopub.status.busy": "2025-03-12T09:07:24.260695Z",
     "iopub.status.idle": "2025-03-12T09:07:57.696975Z",
     "shell.execute_reply": "2025-03-12T09:07:57.696157Z",
     "shell.execute_reply.started": "2025-03-12T09:07:24.260920Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔢 Number of features after Polynomial Features: 120\n",
      "✅  Optimal L1 lambda: 1\n",
      "📊 Best MSE: 0.5431\n",
      "📊 Best RMSE: 0.7081\n",
      "📊 Best MAE: 0.2424\n",
      "📊 R² (Train): 0.5355, Adjusted R² (Train): 0.5316\n",
      "📊 R² (Test): 0.4552, Adjusted R² (Test): 0.3853\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_value = 17\n",
    "random.seed(seed_value)\n",
    "np.random.seed(seed_value)\n",
    "torch.manual_seed(seed_value)\n",
    "torch.cuda.manual_seed_all(seed_value)  # If using GPU\n",
    "\n",
    "# Define TimeSeriesSplit (e.g., 5 splits)\n",
    "panel_split = PanelSplit(periods = final_df.year, n_splits = 5)\n",
    "\n",
    "# Hyperparameters\n",
    "lambda_values = [0.00001, 0.0001, 0.001, 0.01, 0.05, 1]  # Different L1 values\n",
    "epochs = 1000\n",
    "\n",
    "\n",
    "# Adjusted R² function\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "# Dictionary to store results across all CV splits\n",
    "results = {}\n",
    "\n",
    "fold_results = []  # Store results for each fold\n",
    "\n",
    "# Perform TimeSeriesSplit Cross-Validation\n",
    "for train_idx, test_idx in panel_split.split(X1):\n",
    "\n",
    "    # Create interaction terms (degree=2 means pairwise interactions)\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    X_interactions = poly.fit_transform(X1)\n",
    "    \n",
    "    # Split dataset into train & test per fold\n",
    "    X_train1, X_test1 = X_interactions[train_idx], X_interactions[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Convert to PyTorch tensors\n",
    "    X_train_tensor1 = torch.tensor(X_train1, dtype=torch.float32).to(device)\n",
    "    X_test_tensor1 = torch.tensor(X_test1, dtype=torch.float32).to(device)\n",
    "    y_tensor_train = torch.tensor(y_train, dtype=torch.float32).to(device)\n",
    "    y_tensor_test = torch.tensor(y_test, dtype=torch.float32).to(device)\n",
    "\n",
    "    # Train and evaluate for each lambda value\n",
    "    for l1_lambda in lambda_values:\n",
    "        model = LassoRegression(X_train_tensor1.shape[1], l1_lambda=l1_lambda).to(device)\n",
    "        optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "        criterion = nn.MSELoss()\n",
    "\n",
    "        # Training Loop\n",
    "        for epoch in range(epochs):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(X_train_tensor1)\n",
    "            loss = criterion(y_pred, y_tensor_train) + model.l1_regularization_loss()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # Evaluate on train & test sets\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_train_pred = model(X_train_tensor1)  # Training predictions\n",
    "            y_test_pred = model(X_test_tensor1)   # Test predictions\n",
    "\n",
    "        # Convert predictions to NumPy for evaluation\n",
    "        y_train_pred_numpy = y_train_pred.cpu().numpy().flatten()\n",
    "        y_test_pred_numpy = y_test_pred.cpu().numpy().flatten()\n",
    "        y_train_numpy = y_tensor_train.cpu().numpy().flatten()\n",
    "        y_test_numpy = y_tensor_test.cpu().numpy().flatten()\n",
    "\n",
    "        # Compute metrics\n",
    "        mse = mean_squared_error(y_test_numpy, y_test_pred_numpy)\n",
    "        rmse = np.sqrt(mse)\n",
    "        mae = mean_absolute_error(y_test_numpy, y_test_pred_numpy)\n",
    "        r2_test = r2_score(y_test_numpy, y_test_pred_numpy)  # Test R²\n",
    "        r2_train = r2_score(y_train_numpy, y_train_pred_numpy)  # Train R²\n",
    "\n",
    "        # Compute Adjusted R²\n",
    "        n_train, k = X_train1.shape\n",
    "        n_test = X_test1.shape[0]\n",
    "\n",
    "        adj_r2_train = adjusted_r2(r2_train, n_train, k)\n",
    "        adj_r2_test = adjusted_r2(r2_test, n_test, k)\n",
    "\n",
    "        # Store results for this fold\n",
    "        fold_results.append({\n",
    "            \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2_test\": r2_test, \"R2_train\": r2_train,\n",
    "            \"Adj_R2_test\": adj_r2_test, \"Adj_R2_train\": adj_r2_train\n",
    "        })\n",
    "\n",
    "# Compute average performance across all folds for each (k, lambda)\n",
    "avg_results = {\n",
    "    metric: np.mean([fold[metric] for fold in fold_results])\n",
    "    for metric in fold_results[0].keys()\n",
    "}\n",
    "results[(l1_lambda)] = avg_results\n",
    "\n",
    "# Find the best lambda\n",
    "best_lambda = min(results, key=lambda x: (results[x][\"MSE\"], -results[x][\"R2_test\"]))  # Minimize MSE, maximize R²\n",
    "best_metrics = results[best_lambda]\n",
    "\n",
    "# Print optimal hyperparameters and their performance\n",
    "print(f\"🔢 Number of features after Polynomial Features: {X_interactions.shape[1]}\")\n",
    "print(f\"✅  Optimal L1 lambda: {best_lambda}\")\n",
    "print(f\"📊 Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"📊 Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"📊 Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"📊 R² (Train): {best_metrics['R2_train']:.4f}, Adjusted R² (Train): {best_metrics['Adj_R2_train']:.4f}\")\n",
    "print(f\"📊 R² (Test): {best_metrics['R2_test']:.4f}, Adjusted R² (Test): {best_metrics['Adj_R2_test']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. e"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:57.699006Z",
     "iopub.status.busy": "2025-03-12T09:07:57.698765Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit  # Alternative to PanelSplit\n",
    "\n",
    "# Set seed for reproducibility\n",
    "seed_value = 17\n",
    "np.random.seed(seed_value)\n",
    "\n",
    "# Define Time-Series Cross-Validation (5 splits)\n",
    "tscv = TimeSeriesSplit(n_splits=5)\n",
    "\n",
    "# Hyperparameters for Random Forest\n",
    "n_estimators_values = [50, 100, 200, 300]  # Number of trees to test\n",
    "max_depth_values = [5, 10, 20, None]  # Depth of trees\n",
    "\n",
    "# Function to compute Adjusted R²\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "# Dictionary to store results\n",
    "results = {}\n",
    "\n",
    "# Perform Time-Series Cross-Validation\n",
    "for train_idx, test_idx in tscv.split(X1):\n",
    "    \n",
    "    # Create polynomial interaction terms (degree=2, only interactions)\n",
    "    poly = PolynomialFeatures(degree=2, interaction_only=True, include_bias=False)\n",
    "    X_interactions = poly.fit_transform(X1)\n",
    "\n",
    "    # Split dataset into train & test per fold\n",
    "    X_train1, X_test1 = X_interactions[train_idx], X_interactions[test_idx]\n",
    "    y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "    # Train and evaluate for each combination of hyperparameters\n",
    "    for n_estimators in n_estimators_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            \n",
    "            # Define the Random Forest model\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=seed_value,\n",
    "                n_jobs=-1  # Use all available processors\n",
    "            )\n",
    "\n",
    "            # Train the model\n",
    "            model.fit(X_train1, y_train.ravel())\n",
    "\n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X_train1)\n",
    "            y_test_pred = model.predict(X_test1)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            mse = mean_squared_error(y_test, y_test_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "\n",
    "            # Compute Adjusted R²\n",
    "            n_train, k = X_train1.shape\n",
    "            n_test = X_test1.shape[0]\n",
    "            adj_r2_train = adjusted_r2(r2_train, n_train, k)\n",
    "            adj_r2_test = adjusted_r2(r2_test, n_test, k)\n",
    "\n",
    "            # Store results for this combination\n",
    "            results[(n_estimators, max_depth)] = {\n",
    "                \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2_test\": r2_test, \"R2_train\": r2_train,\n",
    "                \"Adj_R2_test\": adj_r2_test, \"Adj_R2_train\": adj_r2_train\n",
    "            }\n",
    "\n",
    "# Find the best hyperparameter combination (minimize MSE, maximize R²)\n",
    "best_params = min(results, key=lambda x: (results[x][\"MSE\"], -results[x][\"R2_test\"]))\n",
    "best_metrics = results[best_params]\n",
    "\n",
    "# Print optimal hyperparameters and their performance\n",
    "print(f\"🌲 Optimal Random Forest Parameters: n_estimators={best_params[0]}, max_depth={best_params[1]}\")\n",
    "print(f\"📊 Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"📊 Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"📊 Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"📊 R² (Train): {best_metrics['R2_train']:.4f}, Adjusted R² (Train): {best_metrics['Adj_R2_train']:.4f}\")\n",
    "print(f\"📊 R² (Test): {best_metrics['R2_test']:.4f}, Adjusted R² (Test): {best_metrics['Adj_R2_test']:.4f}\")\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6837566,
     "sourceId": 10985952,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "MDS_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
