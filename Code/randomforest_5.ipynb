{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Roandom Forest"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.1. Install packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:11.635852Z",
     "iopub.status.busy": "2025-03-12T09:07:11.635460Z",
     "iopub.status.idle": "2025-03-12T09:07:19.489740Z",
     "shell.execute_reply": "2025-03-12T09:07:19.488687Z",
     "shell.execute_reply.started": "2025-03-12T09:07:11.635823Z"
    },
    "trusted": true
   },
   "outputs": [],
   "source": [
    "#!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu118\n",
    "#!pip install panelsplit\n",
    "#!pip install openpyxl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.2. Load packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-03-12T09:07:19.491696Z",
     "iopub.status.busy": "2025-03-12T09:07:19.491359Z",
     "iopub.status.idle": "2025-03-12T09:07:19.497614Z",
     "shell.execute_reply": "2025-03-12T09:07:19.496644Z",
     "shell.execute_reply.started": "2025-03-12T09:07:19.491669Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# from sklearn.model_selection import KFold\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.feature_selection import SelectKBest, f_regression, mutual_info_regression\n",
    "import random\n",
    "\n",
    "from panelsplit.cross_validation import PanelSplit\n",
    "\n",
    "import zipfile\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import openpyxl\n",
    "\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.model_selection import TimeSeriesSplit  # Alternative to PanelSplit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.3. Activate GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA Available: False\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 3\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mtorch\u001b[39;00m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCUDA Available:\u001b[39m\u001b[33m\"\u001b[39m, torch.cuda.is_available())\n\u001b[32m----> \u001b[39m\u001b[32m3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mGPU Name:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mcuda\u001b[49m\u001b[43m.\u001b[49m\u001b[43mget_device_name\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m)\u001b[49m)\n\u001b[32m      6\u001b[39m \u001b[38;5;66;03m# Check if GPU is available\u001b[39;00m\n\u001b[32m      7\u001b[39m device = torch.device(\u001b[33m\"\u001b[39m\u001b[33mcuda\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m torch.cuda.is_available() \u001b[38;5;28;01melse\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mcpu\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\torch\\cuda\\__init__.py:491\u001b[39m, in \u001b[36mget_device_name\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    479\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_name\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> \u001b[38;5;28mstr\u001b[39m:\n\u001b[32m    480\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the name of a device.\u001b[39;00m\n\u001b[32m    481\u001b[39m \n\u001b[32m    482\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    489\u001b[39m \u001b[33;03m        str: the name of the device\u001b[39;00m\n\u001b[32m    490\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m491\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mget_device_properties\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m.name\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\torch\\cuda\\__init__.py:523\u001b[39m, in \u001b[36mget_device_properties\u001b[39m\u001b[34m(device)\u001b[39m\n\u001b[32m    511\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mget_device_properties\u001b[39m(device: Optional[_device_t] = \u001b[38;5;28;01mNone\u001b[39;00m) -> _CudaDeviceProperties:\n\u001b[32m    512\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33mr\u001b[39m\u001b[33;03m\"\"\"Get the properties of a device.\u001b[39;00m\n\u001b[32m    513\u001b[39m \n\u001b[32m    514\u001b[39m \u001b[33;03m    Args:\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    521\u001b[39m \u001b[33;03m        _CudaDeviceProperties: the properties of the device\u001b[39;00m\n\u001b[32m    522\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m523\u001b[39m     \u001b[43m_lazy_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# will define _get_device_properties\u001b[39;00m\n\u001b[32m    524\u001b[39m     device = _get_device_index(device, optional=\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[32m    525\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m device < \u001b[32m0\u001b[39m \u001b[38;5;129;01mor\u001b[39;00m device >= device_count():\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\torch\\cuda\\__init__.py:319\u001b[39m, in \u001b[36m_lazy_init\u001b[39m\u001b[34m()\u001b[39m\n\u001b[32m    317\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os.environ:\n\u001b[32m    318\u001b[39m     os.environ[\u001b[33m\"\u001b[39m\u001b[33mCUDA_MODULE_LOADING\u001b[39m\u001b[33m\"\u001b[39m] = \u001b[33m\"\u001b[39m\u001b[33mLAZY\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m--> \u001b[39m\u001b[32m319\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_C\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    320\u001b[39m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[32m    321\u001b[39m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[32m    322\u001b[39m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[32m    323\u001b[39m _tls.is_initializing = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mRuntimeError\u001b[39m: Found no NVIDIA driver on your system. Please check that you have an NVIDIA GPU and installed a driver from http://www.nvidia.com/Download/index.aspx"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(\"CUDA Available:\", torch.cuda.is_available())\n",
    "print(\"GPU Name:\", torch.cuda.get_device_name(0))\n",
    "\n",
    "\n",
    "# Check if GPU is available\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0.4. Load Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Check working directory\n",
    "#print(os.getcwd()) \n",
    "\n",
    "### Define file and path\n",
    "#file_path = r\"c:\\Users\\mmier\\OneDrive - Hertie School\\3. Estudio\\2025 MDS\\2025-1 MDS Thesis\\MDS_thesis\\Data\" #Use a raw string (r\"\") when defining paths\n",
    "#print(os.listdir(file_path)) # List files in directory\n",
    "csv_filename = \"../final_df.csv\"\n",
    "zip_filename = \"../final_df.zip\"\n",
    "#full_path = os.path.join(file_path, csv_filename)\n",
    "\n",
    "# Extract the CSV file from the ZIP\n",
    "with zipfile.ZipFile(zip_filename, 'r') as zipf:\n",
    "    zipf.extract(csv_filename)\n",
    "\n",
    "# Read the CSV file into a DataFrame\n",
    "final_df = pd.read_csv(\"final_df.csv\")\n",
    "\n",
    "# Delete the extracted CSV file after reading\n",
    "os.remove(\"final_df.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1. Define train, test and evaluation set\n",
    "\n",
    "Evaluation set: 2019 to 2022"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "### Separate evaluation set \n",
    "#final_df = df[df[\"AÑO\"] < 2019]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2. Define y and X1\n",
    "\n",
    "y: deforestation\n",
    "\n",
    "X1: general variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# y \n",
    "y = final_df['pc_loss']\n",
    "y = np.log1p(final_df['pc_loss'])\n",
    "#y = y.to_numpy()\n",
    "\n",
    "# Normalize output\n",
    "#scaler = StandardScaler()\n",
    "\n",
    "# X final\n",
    "X1 = final_df.drop(columns=['year', 'codmpio', 'cluster_kmeans', 'Departamento', 'Municipio', 'Region', 'pc_loss', 'f_loss', 'areaoficialhm2', 'gdp']) # GDP, area already out since the were already considered\n",
    "X1 = X1.drop(columns=['CV-01-15','CV-03-64', 'CV-03-26b', 'CV-01-11', 'CV-01-1', 'PCC-02-0', 'PCC-02-3', 'EIS-00-0', 'PCC-00-0', 'CTI-00-0', 'C-02-8t', 'ICM-00-0',\n",
    "                      'EIS-03-4', 'CTI-01-3', 'SEG-00-0', 'SEG-01-6', 'SOS-00-0', 'SOS-02-0', 'SOS-02-2', 'SOS-01-6', 'GPI-00-0', 'GPI-02-4', 'GPI-02-5', 'P-01-34-1',\n",
    "                      'P-01-46', 'P-01-25', 'CV-02-12e', 'CV-03-51', 'total_ac']) \n",
    "\n",
    "\n",
    "# Keep feature names\n",
    "original_feature_names = list(X1.columns) \n",
    "\n",
    "# Normalize features\n",
    "#X1 = scaler.fit_transform(X1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Panel Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1. Define splits "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from panelsplit.cross_validation import PanelSplit\n",
    "\n",
    "### Initialize PanelSplit\n",
    "panel_split = PanelSplit(periods=final_df.year, n_splits=5)\n",
    "splits = panel_split.split()\n",
    "\n",
    "### Check splits, for n_splits=3\n",
    "splits_1_train = final_df.loc[splits[0][0]]\n",
    "splits_1_test = final_df.loc[splits[0][1]]\n",
    "splits_2_train = final_df.loc[splits[1][0]]\n",
    "splits_2_test = final_df.loc[splits[1][1]]\n",
    "splits_3_train = final_df.loc[splits[2][0]]\n",
    "splits_3_test = final_df.loc[splits[2][1]]\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.2. Train a Quick Random Forest to Identify Important Features\n",
    "\n",
    "Before running the full model, we train a baseline Random Forest to rank feature importance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "          Feature  Importance\n",
      "13         H_coca    0.218206\n",
      "5       disbogota    0.069497\n",
      "14   errad_manual    0.041283\n",
      "67        C-03-14    0.031512\n",
      "23      e_tortura    0.030593\n",
      "60      INS-01-8c    0.026947\n",
      "3      discapital    0.025323\n",
      "25  eventos_minas    0.025114\n",
      "55       GPI-03-0    0.023562\n",
      "36       SOS-03-6    0.021766\n",
      "63       PCC-01-0    0.021737\n",
      "65       PCC-01-2    0.019785\n",
      "21     e_desplaza    0.019697\n",
      "7        pobl_urb    0.019636\n",
      "4          dismdo    0.019216\n",
      "69       PCC-02-4    0.017593\n",
      "47      CV-03-31e    0.016529\n",
      "33        M-03-15    0.016204\n",
      "37       CV-03-16    0.015866\n",
      "27       CTI-02-0    0.014098\n",
      "10        total_r    0.013541\n",
      "44       EIS-01-0    0.012518\n",
      "53       GPI-01-0    0.011789\n",
      "0          gdp_pc    0.011556\n",
      "11      total_nuf    0.011178\n",
      "8        total_as    0.010622\n",
      "50       CV-03-26    0.010504\n",
      "52       EIS-03-3    0.010422\n",
      "12      total_vrf    0.010028\n",
      "1            iica    0.010016\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "import pandas as pd\n",
    "\n",
    "# Define a baseline Random Forest model\n",
    "rf_baseline = RandomForestRegressor(\n",
    "    n_estimators=50,  # Small number of trees for quick evaluation\n",
    "    max_depth=10,     # Limit tree depth to avoid overfitting\n",
    "    random_state=17,\n",
    "    n_jobs=-1         # Use all CPU cores\n",
    ")\n",
    "\n",
    "# Fit the model on the full dataset (ignoring \"AÑO\" and \"Municipio\" for now)\n",
    "#y = final_df[\"M-03-25\"]\n",
    "#X = final_df.drop(columns=[\"M-03-25\", \"Divipola\", \"AÑO\"])  # Remove non-predictive ID variables\n",
    "rf_baseline.fit(X1, y)\n",
    "\n",
    "# Extract feature importance\n",
    "feature_importance_df = pd.DataFrame({\n",
    "    \"Feature\": X1.columns,\n",
    "    \"Importance\": rf_baseline.feature_importances_\n",
    "}).sort_values(by=\"Importance\", ascending=False)\n",
    "\n",
    "# Display the top 30 features\n",
    "print(feature_importance_df.head(30))\n",
    "#import ace_tools as tools\n",
    "#tools.display_dataframe_to_user(name=\"Top Feature Importance\", dataframe=feature_importance_df.head(3))\n",
    "\n",
    "\n",
    "\n",
    "# Keep only the top 30 most important features\n",
    "#top_features = feature_importance_df[\"Feature\"].head(30).tolist()\n",
    "\n",
    "\n",
    "# Update X1 to only include the top features\n",
    "#X1 = X1[top_features]\n",
    "\n",
    "#print(f\"✅ Selected top {len(top_features)} features for the final model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.3.  Train the Final Random Forest Model\n",
    "\n",
    "\"Manual tunning\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[6]\u001b[39m\u001b[32m, line 37\u001b[39m\n\u001b[32m     29\u001b[39m model = RandomForestRegressor(\n\u001b[32m     30\u001b[39m     n_estimators=n_estimators,\n\u001b[32m     31\u001b[39m     max_depth=max_depth,\n\u001b[32m     32\u001b[39m     random_state=\u001b[32m17\u001b[39m,\n\u001b[32m     33\u001b[39m     n_jobs=-\u001b[32m1\u001b[39m  \u001b[38;5;66;03m# Use all available processors\u001b[39;00m\n\u001b[32m     34\u001b[39m )\n\u001b[32m     36\u001b[39m \u001b[38;5;66;03m# Train the model\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m37\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX1_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     39\u001b[39m \u001b[38;5;66;03m# Predictions\u001b[39;00m\n\u001b[32m     40\u001b[39m y_train_pred = model.predict(X1_train)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\sklearn\\ensemble\\_forest.py:487\u001b[39m, in \u001b[36mBaseForest.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m    476\u001b[39m trees = [\n\u001b[32m    477\u001b[39m     \u001b[38;5;28mself\u001b[39m._make_estimator(append=\u001b[38;5;28;01mFalse\u001b[39;00m, random_state=random_state)\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_more_estimators)\n\u001b[32m    479\u001b[39m ]\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# Parallel loop: we prefer the threading backend as the Cython code\u001b[39;00m\n\u001b[32m    482\u001b[39m \u001b[38;5;66;03m# for fitting the trees is internally releasing the Python GIL\u001b[39;00m\n\u001b[32m    483\u001b[39m \u001b[38;5;66;03m# making threading more efficient than multiprocessing in\u001b[39;00m\n\u001b[32m    484\u001b[39m \u001b[38;5;66;03m# that case. However, for joblib 0.12+ we respect any\u001b[39;00m\n\u001b[32m    485\u001b[39m \u001b[38;5;66;03m# parallel_backend contexts set at a higher level,\u001b[39;00m\n\u001b[32m    486\u001b[39m \u001b[38;5;66;03m# since correctness does not rely on using threads.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m487\u001b[39m trees = \u001b[43mParallel\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    488\u001b[39m \u001b[43m    \u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mn_jobs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    489\u001b[39m \u001b[43m    \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    490\u001b[39m \u001b[43m    \u001b[49m\u001b[43mprefer\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mthreads\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m    491\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    492\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdelayed\u001b[49m\u001b[43m(\u001b[49m\u001b[43m_parallel_build_trees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    493\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    494\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mbootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    495\u001b[39m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    496\u001b[39m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    497\u001b[39m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    498\u001b[39m \u001b[43m        \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    499\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    500\u001b[39m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    501\u001b[39m \u001b[43m        \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    502\u001b[39m \u001b[43m        \u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m=\u001b[49m\u001b[43mn_samples_bootstrap\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    503\u001b[39m \u001b[43m        \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    504\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    505\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mt\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43menumerate\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mtrees\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    506\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    508\u001b[39m \u001b[38;5;66;03m# Collect newly grown trees\u001b[39;00m\n\u001b[32m    509\u001b[39m \u001b[38;5;28mself\u001b[39m.estimators_.extend(trees)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\sklearn\\utils\\parallel.py:77\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m     72\u001b[39m config = get_config()\n\u001b[32m     73\u001b[39m iterable_with_config = (\n\u001b[32m     74\u001b[39m     (_with_config(delayed_func, config), args, kwargs)\n\u001b[32m     75\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m delayed_func, args, kwargs \u001b[38;5;129;01min\u001b[39;00m iterable\n\u001b[32m     76\u001b[39m )\n\u001b[32m---> \u001b[39m\u001b[32m77\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[34;43m__call__\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43miterable_with_config\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\joblib\\parallel.py:2007\u001b[39m, in \u001b[36mParallel.__call__\u001b[39m\u001b[34m(self, iterable)\u001b[39m\n\u001b[32m   2001\u001b[39m \u001b[38;5;66;03m# The first item from the output is blank, but it makes the interpreter\u001b[39;00m\n\u001b[32m   2002\u001b[39m \u001b[38;5;66;03m# progress until it enters the Try/Except block of the generator and\u001b[39;00m\n\u001b[32m   2003\u001b[39m \u001b[38;5;66;03m# reaches the first `yield` statement. This starts the asynchronous\u001b[39;00m\n\u001b[32m   2004\u001b[39m \u001b[38;5;66;03m# dispatch of the tasks to the workers.\u001b[39;00m\n\u001b[32m   2005\u001b[39m \u001b[38;5;28mnext\u001b[39m(output)\n\u001b[32m-> \u001b[39m\u001b[32m2007\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m output \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.return_generator \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;43mlist\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43moutput\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\joblib\\parallel.py:1650\u001b[39m, in \u001b[36mParallel._get_outputs\u001b[39m\u001b[34m(self, iterator, pre_dispatch)\u001b[39m\n\u001b[32m   1647\u001b[39m     \u001b[38;5;28;01myield\u001b[39;00m\n\u001b[32m   1649\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mself\u001b[39m._backend.retrieval_context():\n\u001b[32m-> \u001b[39m\u001b[32m1650\u001b[39m         \u001b[38;5;28;01myield from\u001b[39;00m \u001b[38;5;28mself\u001b[39m._retrieve()\n\u001b[32m   1652\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mGeneratorExit\u001b[39;00m:\n\u001b[32m   1653\u001b[39m     \u001b[38;5;66;03m# The generator has been garbage collected before being fully\u001b[39;00m\n\u001b[32m   1654\u001b[39m     \u001b[38;5;66;03m# consumed. This aborts the remaining tasks if possible and warn\u001b[39;00m\n\u001b[32m   1655\u001b[39m     \u001b[38;5;66;03m# the user if necessary.\u001b[39;00m\n\u001b[32m   1656\u001b[39m     \u001b[38;5;28mself\u001b[39m._exception = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mmier\\anaconda3\\envs\\MDS_thesis\\Lib\\site-packages\\joblib\\parallel.py:1762\u001b[39m, in \u001b[36mParallel._retrieve\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1757\u001b[39m \u001b[38;5;66;03m# If the next job is not ready for retrieval yet, we just wait for\u001b[39;00m\n\u001b[32m   1758\u001b[39m \u001b[38;5;66;03m# async callbacks to progress.\u001b[39;00m\n\u001b[32m   1759\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ((\u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m._jobs) == \u001b[32m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m\n\u001b[32m   1760\u001b[39m     (\u001b[38;5;28mself\u001b[39m._jobs[\u001b[32m0\u001b[39m].get_status(\n\u001b[32m   1761\u001b[39m         timeout=\u001b[38;5;28mself\u001b[39m.timeout) == TASK_PENDING)):\n\u001b[32m-> \u001b[39m\u001b[32m1762\u001b[39m     \u001b[43mtime\u001b[49m\u001b[43m.\u001b[49m\u001b[43msleep\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m0.01\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m   1763\u001b[39m     \u001b[38;5;28;01mcontinue\u001b[39;00m\n\u001b[32m   1765\u001b[39m \u001b[38;5;66;03m# We need to be careful: the job list can be filling up as\u001b[39;00m\n\u001b[32m   1766\u001b[39m \u001b[38;5;66;03m# we empty it and Python list are not thread-safe by\u001b[39;00m\n\u001b[32m   1767\u001b[39m \u001b[38;5;66;03m# default hence the use of the lock\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "### Adjusted R² function\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "### Define hyperparameters for tuning\n",
    "n_estimators_values = [50, 100, 200, 300]  # Number of decision trees in the Random Forest model\n",
    "max_depth_values = [5, 10, 20, None]       # Maximum depth of each tree (controls complexity)\n",
    "\n",
    "### Initialize Storage for Results\n",
    "results = {}\n",
    "\n",
    "### Setup panle data cross validation with  PanelSplit\n",
    "panel_split = PanelSplit(periods=final_df.year, n_splits=3, gap=0, test_size=1)\n",
    "groups = X1.index  # Using index since Municipio was dropped\n",
    "\n",
    "### Loop through Cross-Validation Splits\n",
    "for train_idx, test_idx in panel_split.split(X1, y):\n",
    "#panel_split.split(X, y) iterates through each fold, providing indices for training and testing\n",
    "    \n",
    "    ## Split data\n",
    "    X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "\n",
    "    ## Loop over hyperparameters combination\n",
    "    for n_estimators in n_estimators_values:\n",
    "        for max_depth in max_depth_values:\n",
    "            \n",
    "            # Define the Random Forest model\n",
    "            model = RandomForestRegressor(\n",
    "                n_estimators=n_estimators,\n",
    "                max_depth=max_depth,\n",
    "                random_state=17,\n",
    "                n_jobs=-1  # Use all available processors\n",
    "            )\n",
    "            \n",
    "            # Train the model\n",
    "            model.fit(X1_train, y_train)\n",
    "\n",
    "            # Predictions\n",
    "            y_train_pred = model.predict(X1_train)\n",
    "            y_test_pred = model.predict(X1_test)\n",
    "\n",
    "            # Compute performance metrics\n",
    "            from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "            mse = mean_squared_error(y_test, y_test_pred)\n",
    "            rmse = np.sqrt(mse)\n",
    "            mae = mean_absolute_error(y_test, y_test_pred)\n",
    "            r2_test = r2_score(y_test, y_test_pred)\n",
    "            r2_train = r2_score(y_train, y_train_pred)\n",
    "            # Compute Adjusted R²\n",
    "            n_train, k = X1_train.shape\n",
    "            n_test = X1_test.shape[0]\n",
    "            adj_r2_train = adjusted_r2(r2_train, n_train, k)\n",
    "            adj_r2_test = adjusted_r2(r2_test, n_test, k)\n",
    "\n",
    "            # Store results\n",
    "            if (n_estimators, max_depth) not in results:\n",
    "                results[(n_estimators, max_depth)] = []\n",
    "\n",
    "            results[(n_estimators, max_depth)].append({\n",
    "                \"MSE\": mse, \"RMSE\": rmse, \"MAE\": mae, \"R2_test\": r2_test, \"R2_train\": r2_train, \"Adj_R2_test\": adj_r2_test, \"Adj_R2_train\": adj_r2_train\n",
    "            })\n",
    "\n",
    "### Compute Average Performance Across Folds\n",
    "averaged_results = {\n",
    "    params: {metric: np.mean([fold[metric] for fold in folds])\n",
    "             for metric in folds[0].keys()}\n",
    "    for params, folds in results.items()\n",
    "}\n",
    "\n",
    "### Find the best hyperparameter combination\n",
    "best_params = min(averaged_results, key=lambda x: (averaged_results[x][\"MSE\"], -averaged_results[x][\"R2_test\"]))\n",
    "best_metrics = averaged_results[best_params]\n",
    "\n",
    "### Print optimal hyperparameters and performance\n",
    "print(f\"🌲 Best Random Forest Parameters: n_estimators={best_params[0]}, max_depth={best_params[1]}\")\n",
    "print(f\"📊 Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"📊 Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"📊 Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"📊 R² (Train): {best_metrics['R2_train']:.4f}\")\n",
    "print(f\"📊 R² (Test): {best_metrics['R2_test']:.4f}\")\n",
    "print(f\"📊 Adj. R² (Train): {best_metrics['Adj_R2_train']:.4f}\")\n",
    "print(f\"📊 Adj. R² (Test): {best_metrics['Adj_R2_test']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "🌲 Best Random Forest Parameters: n_estimators=300, max_depth=5\n",
    "📊 Best MSE: 739947.8996\n",
    "📊 Best RMSE: 839.6786\n",
    "📊 Best MAE: 174.8199\n",
    "📊 R² (Train): 0.2308\n",
    "📊 R² (Test): 0.0281\n",
    "📊 Adj. R² (Train): 0.2303\n",
    "📊 Adj. R² (Test): 0.0220"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 180 candidates, totalling 540 fits\n",
    "🌲 Best Random Forest Parameters: n_estimators=700, max_depth=None\n",
    "📊 Best MSE: 709110.1742\n",
    "📊 Best RMSE: 821.6917\n",
    "📊 Best MAE: 170.5814\n",
    "📊 R² (Train): 0.4968\n",
    "📊 R² (Test): 0.0701\n",
    "📊 Adj. R² (Train): 0.4965\n",
    "📊 Adj. R² (Test): 0.0642 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Search random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
      "⏱️ Total tuning time: 2544.35 seconds\n",
      "🌲 Best Random Forest Parameters: n_estimators=714, max_depth=20\n",
      "📊 Best MSE: 0.0044\n",
      "📊 Best RMSE: 0.0656\n",
      "📊 Best MAE: 0.0340\n",
      "📊 R² (Train): 0.8835\n",
      "📊 R² (Test): 0.6562\n",
      "📊 Adj. R² (Train): 0.8828\n",
      "📊 Adj. R² (Test): 0.6332\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "### Adjusted R² function\n",
    "def adjusted_r2(r2, n, k):\n",
    "    return 1 - (1 - r2) * ((n - 1) / (n - k - 1))\n",
    "\n",
    "### Define hyperparameter space\n",
    "param_distributions = {\n",
    "    \"n_estimators\": np.random.randint(100, 1000, 10).tolist(),  # Random values from 100 to 1000\n",
    "    \"max_depth\": [3, 5, 10, 20, None],  \n",
    "    \"min_samples_split\": np.random.randint(2, 15, 5).tolist(),  \n",
    "    \"min_samples_leaf\": np.random.randint(1, 10, 5).tolist()\n",
    "}\n",
    "\n",
    "### Setup panel data cross-validation\n",
    "panel_split = PanelSplit(periods=final_df.year, n_splits=3, gap=0, test_size=1)\n",
    "groups = X1.index  # Using index since Municipio was dropped\n",
    "\n",
    "# Define the RandomForestRegressor model\n",
    "model = RandomForestRegressor(random_state=17, n_jobs=-1)\n",
    "\n",
    "# Setup RandomizedSearchCV with PanelSplit cross-validation\n",
    "random_search = RandomizedSearchCV(\n",
    "    estimator=model,\n",
    "    param_distributions=param_distributions,\n",
    "    n_iter=30,  # Number of random samples to try\n",
    "    scoring=\"neg_mean_squared_error\",  # Minimize MSE\n",
    "    cv=panel_split.split(X1, y),  # Custom panel CV\n",
    "    n_jobs=-1,  # Parallel computation\n",
    "    verbose=2,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Run random search\n",
    "start = time.time()\n",
    "random_search.fit(X1, y)\n",
    "print(f\"⏱️ Total tuning time: {time.time() - start:.2f} seconds\")\n",
    "\n",
    "# Extract best model and hyperparameters\n",
    "best_params = random_search.best_params_\n",
    "best_model = random_search.best_estimator_\n",
    "\n",
    "# Store results for best hyperparameters\n",
    "results = []\n",
    "\n",
    "# Iterate through each fold for evaluation\n",
    "for train_idx, test_idx in panel_split.split(X1, y):\n",
    "    # Split data\n",
    "    X1_train, X1_test = X1.iloc[train_idx], X1.iloc[test_idx]\n",
    "    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
    "    \n",
    "    # Train the best model on the current fold\n",
    "    best_model.fit(X1_train, y_train)\n",
    "\n",
    "    # Make predictions\n",
    "    y_train_pred = best_model.predict(X1_train)\n",
    "    y_test_pred = best_model.predict(X1_test)\n",
    "\n",
    "    # Compute performance metrics\n",
    "    mse_train = mean_squared_error(y_train, y_train_pred)\n",
    "    mse_test = mean_squared_error(y_test, y_test_pred)\n",
    "    rmse_train = np.sqrt(mse_train)\n",
    "    rmse_test = np.sqrt(mse_test)\n",
    "    mae_train = mean_absolute_error(y_train, y_train_pred)\n",
    "    mae_test = mean_absolute_error(y_test, y_test_pred)\n",
    "    r2_train = r2_score(y_train, y_train_pred)\n",
    "    r2_test = r2_score(y_test, y_test_pred)\n",
    "\n",
    "    # Compute Adjusted R²\n",
    "    n_train, k = X1_train.shape\n",
    "    n_test = X1_test.shape[0]\n",
    "    adj_r2_train = adjusted_r2(r2_train, n_train, k)\n",
    "    adj_r2_test = adjusted_r2(r2_test, n_test, k)\n",
    "\n",
    "    # Store results\n",
    "    results.append({\n",
    "        \"MSE\": mse_test,\n",
    "        \"RMSE\": rmse_test,\n",
    "        \"MAE\": mae_test,\n",
    "        \"R2_train\": r2_train,\n",
    "        \"R2_test\": r2_test,\n",
    "        \"Adj_R2_train\": adj_r2_train,\n",
    "        \"Adj_R2_test\": adj_r2_test\n",
    "    })\n",
    "\n",
    "# Compute average performance across folds\n",
    "best_metrics = {metric: np.mean([fold[metric] for fold in results]) for metric in results[0].keys()}\n",
    "\n",
    "### Print optimal hyperparameters and performance\n",
    "print(f\"🌲 Best Random Forest Parameters: n_estimators={best_params['n_estimators']}, max_depth={best_params['max_depth']}\")\n",
    "print(f\"📊 Best MSE: {best_metrics['MSE']:.4f}\")\n",
    "print(f\"📊 Best RMSE: {best_metrics['RMSE']:.4f}\")\n",
    "print(f\"📊 Best MAE: {best_metrics['MAE']:.4f}\")\n",
    "print(f\"📊 R² (Train): {best_metrics['R2_train']:.4f}\")\n",
    "print(f\"📊 R² (Test): {best_metrics['R2_test']:.4f}\")\n",
    "print(f\"📊 Adj. R² (Train): {best_metrics['Adj_R2_train']:.4f}\")\n",
    "print(f\"📊 Adj. R² (Test): {best_metrics['Adj_R2_test']:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fitting 3 folds for each of 30 candidates, totalling 90 fits\n",
    "🌲 Best Random Forest Parameters: n_estimators=853, max_depth=20\n",
    "📊 Best MSE: 706687.1554\n",
    "📊 Best RMSE: 819.8642\n",
    "📊 Best MAE: 168.8173\n",
    "📊 R² (Train): 0.3132\n",
    "📊 R² (Test): 0.0754\n",
    "📊 Adj. R² (Train): 0.3128\n",
    "📊 Adj. R² (Test): 0.0696"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 6837566,
     "sourceId": 10985952,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 30919,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "MDS_thesis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
